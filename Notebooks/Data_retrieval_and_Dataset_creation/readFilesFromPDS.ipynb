{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44936e52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#expand cell width to 100%\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b997ca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the address of the directory containing the grs files\n",
    "grsFilesDirectoryAddress=\"https://pds-geosciences.wustl.edu/lunar/lp-l-grs-3-rdr-v1/lp_2xxx/grs/\"\n",
    "#set the url-prefix that is to be appended before the relative address of each file\n",
    "absoluteURLPrefix=\"https://pds-geosciences.wustl.edu/\"\n",
    "#set the directory where the files will be stored locally\n",
    "localFilesDirectory=\"D:/Non-academic/GSOC23/Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e7b59e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests\n",
    "import requests\n",
    "#import beautiful soup\n",
    "from bs4 import BeautifulSoup\n",
    "#make a get request and get the html file (web-page) at the grs file directory address\n",
    "grsFilesDirectoryPage=requests.get(grsFilesDirectoryAddress)\n",
    "#parse the page with beautiful soup and get the body\n",
    "grsDirectoryBody=BeautifulSoup(grsFilesDirectoryPage.content,'html.parser').body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "664d15bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract all the links in the body of the page as a list\n",
    "#drop the first element in the list as it links back to the parent directory\n",
    "#drop the last two elements as they are an .xml file describing the dataset and a .csv inventory file, respectively \n",
    "allLinks=grsDirectoryBody.select('a')[1:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "689abe81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lists to hold the .dat and .lbl file addresses as well as the year and day as YYYY_DDD\n",
    "datFileAddresses=[]\n",
    "lblFileAddresses=[]\n",
    "xmlFileAddresses=[]\n",
    "yearAndDayKeys=[]\n",
    "#iterate through all the links and seperate out the .dat, .lbl and .xml files addresses\n",
    "for currentLink in allLinks:\n",
    "    #get the text for the link\n",
    "    linkText=currentLink.text\n",
    "    #check wether the link is that of a .dat, .lbl, or .xml and proceed accordingly\n",
    "    if linkText[linkText.index('.'):]=='.dat':\n",
    "        datFileAddresses.append(currentLink.get('href'))\n",
    "        yearAndDayKeys.append(linkText[:linkText.index('.')])\n",
    "    elif linkText[linkText.index('.'):]=='.lbl':\n",
    "        lblFileAddresses.append(currentLink.get('href'))\n",
    "    #if links aren't for .dat or .lbl they are .xml\n",
    "    else:\n",
    "        xmlFileAddresses.append(currentLink.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f0e7457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function which given the the web-address of a file saves it to the disk and returns the disk-address\n",
    "def saveFile(relativeFileURL):\n",
    "    #get the prefixes\n",
    "    global absoluteURLPrefix\n",
    "    global localFilesDirectory\n",
    "    #get the absolute URL of the file\n",
    "    absoluteFileURL=absoluteURLPrefix+relativeFileURL\n",
    "    #get the name of the file\n",
    "    fileName=relativeFileURL[relativeFileURL.rindex('/')+1:]\n",
    "    #get the absolute address of the file on the disk\n",
    "    absoluteFileAddress=localFilesDirectory+fileName\n",
    "    #write the file to disk\n",
    "    with requests.get(absoluteFileURL,stream=True) as currentRequest:\n",
    "        currentRequest.raise_for_status()\n",
    "        with open(absoluteFileAddress, 'wb') as currentFile:\n",
    "            for chunk in currentRequest.iter_content(chunk_size=8192):  \n",
    "                currentFile.write(chunk)\n",
    "    return absoluteFileAddress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abb9359a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import minidom from xml.dom\n",
    "from xml.dom import minidom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5d68e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to read the xml file at the given address and get an ordered list of indices corresponding to columns to be read from the .dat file\n",
    "def readColumnIndicesFromXMLFile(xmlFileAddress):\n",
    "    #read the xml file\n",
    "    xmlFile=minidom.parse(xmlFileAddress)\n",
    "    #get the elements describing the columns of the .dat file\n",
    "    columnElements=xmlFile.getElementsByTagName('Field_Binary')\n",
    "    #create an empty list to hold the indices of the columns to be read\n",
    "    requiredColumnIndices=[]\n",
    "    #iterate over the names of the fields in the current data record\n",
    "    for i in range(len(columnElements)):\n",
    "        #iterate over the required fields\n",
    "        for j in requiredFields:\n",
    "            #if the current field is required save its index\n",
    "            if columnElements[i].getElementsByTagName('name')[0].firstChild.nodeValue==j:\n",
    "                requiredColumnIndices.append(i)\n",
    "    return requiredColumnIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ed89798",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy\n",
    "import numpy as np\n",
    "#import struct to convert byte to float32\n",
    "import struct\n",
    "#import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1927a000",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate over all .dat .xml file combinations\n",
    "for currentDatFileURL,currentxmlFileURL in zip(datFileAddresses,xmlFileAddresses):\n",
    "    #download and read the .dat file\n",
    "    datFileAddress=saveFile(currentDatFileURL)\n",
    "    #download and read the .xml file\n",
    "    xmlFileAddress=saveFile(currentxmlFileURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf48258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function which takes the addresses of a .dat file and its associated .xml file\n",
    "#reads the parameters from the .xml file uses it to read the spectra from the .dat file\n",
    "#returns a dataframe where each row corresponds to each record in the .dat file\n",
    "def readGRSData(datFileAddress,xmlFileAddress):\n",
    "    #create a list containing the fields to be read from the .dat file\n",
    "    requiredFields=['Spacecraft_Altitude','Subspacecraft_Latitude','Subspacecraft_Longitude']\n",
    "    #read the xml file\n",
    "    xmlFile=minidom.parse(xmlFileAddress)\n",
    "    #get the elements describing the columns of the .dat file\n",
    "    columnElements=xmlFile.getElementsByTagName('Field_Binary')\n",
    "    #create an empty list to hold the indices of the columns to be read\n",
    "    requiredColumnIndices=[]\n",
    "    #create an empty list to hold the no. of bytes per element for each column to be read\n",
    "    bytesPerItermPerColumn=[]\n",
    "    #create an empty list to hold the number of the starting position for each column\n",
    "    startingBytePerColumn=[]\n",
    "    #iterate over the names of the fields in the current data record\n",
    "    for i in range(len(columnElements)):\n",
    "        #iterate over the required fields\n",
    "        for j in requiredFields:\n",
    "            #if the current field is required save its index, starting bytes, and length in bytes\n",
    "            if columnElements[i].getElementsByTagName('name')[0].firstChild.nodeValue==j:\n",
    "                requiredColumnIndices.append(i)\n",
    "                startingBytePerColumn.append(int(columnElements[i].getElementsByTagName('field_location')[0].firstChild.nodeValue))\n",
    "                bytesPerItermPerColumn.append(int(columnElements[i].getElementsByTagName('field_length')[0].firstChild.nodeValue))\n",
    "\n",
    "    #create two dictionaries marking the starting byte and length for the accepted and rejected spectra\n",
    "    acceptedSpectraDescription={\"startingByte\":-1,\"noOfBytes\":-1}\n",
    "    rejectedSpectraDescription={\"startingByte\":-1,\"noOfBytes\":-1}\n",
    "    #get element describing the accepted spectra in the .dat file\n",
    "\n",
    "    acceptedSpectraElement=xmlFile.getElementsByTagName('Group_Field_Binary')[0]\n",
    "    #save the starting position\n",
    "    acceptedSpectraDescription[\"startingByte\"]=int(acceptedSpectraElement.getElementsByTagName('group_location')[0].firstChild.nodeValue)\n",
    "    #save the ending position\n",
    "    acceptedSpectraDescription[\"noOfBytes\"]=int(acceptedSpectraElement.getElementsByTagName('group_length')[0].firstChild.nodeValue)\n",
    "\n",
    "    #get element describing the accepted spectra in the .dat file\n",
    "    rejectedSpectraElement=xmlFile.getElementsByTagName('Group_Field_Binary')[1]\n",
    "    #save the starting position\n",
    "    rejectedSpectraDescription[\"startingByte\"]=int(rejectedSpectraElement.getElementsByTagName('group_location')[0].firstChild.nodeValue)\n",
    "    #save the ending position\n",
    "    rejectedSpectraDescription[\"noOfBytes\"]=int(rejectedSpectraElement.getElementsByTagName('group_length')[0].firstChild.nodeValue)\n",
    "    #save the number of records in the .dat file\n",
    "    noOfRecords=int(xmlFile.getElementsByTagName('Table_Binary')[0].getElementsByTagName('records')[0].firstChild.nodeValue)\n",
    "    #save the number of bytes per record\n",
    "    noOfBytesPerRecord=int(xmlFile.getElementsByTagName('Table_Binary')[0].getElementsByTagName('Record_Binary')[0].getElementsByTagName('record_length')[0].firstChild.nodeValue)\n",
    "\n",
    "\n",
    "    #create an empty list to hold each byte of each record in the .data file\n",
    "    datFileBytes=[]\n",
    "    #open the .dat file\n",
    "    with open(datFileAddress,mode='rb') as datFile:\n",
    "        #iterate over each record\n",
    "        for i in range(noOfRecords):\n",
    "            #create a temporary list hold the bytes of each record\n",
    "            currentRecordBytes=[]\n",
    "            #iterate over all bytes in the record\n",
    "            for j in range(noOfBytesPerRecord):\n",
    "                #read each byte of the .dat file seperately into an array\n",
    "                currentRecordBytes.append(datFile.read(1))\n",
    "            #add the current record to the list of bytes\n",
    "            datFileBytes.append(currentRecordBytes)\n",
    "\n",
    "    #set the number of bytes per channel\n",
    "    bytesPerChannel=4\n",
    "    #set the number of channels in the spectra\n",
    "    channelsPerSpectra=int(acceptedSpectraDescription[\"noOfBytes\"]/bytesPerChannel)\n",
    "\n",
    "    #create empty arrays to hold:\n",
    "    #accepted spectra\n",
    "    acceptedSpectras=[]\n",
    "    #rejected spectra\n",
    "    rejectedSpectras=[]\n",
    "    #altitudes\n",
    "    altitudes=[]\n",
    "    #latitudes\n",
    "    latitudes=[]\n",
    "    #longitudes\n",
    "    longitudes=[]\n",
    "\n",
    "    #iterate through all the records\n",
    "    for i in range(len(datFileBytes)):\n",
    "        #get the bytes for the current record\n",
    "        currentRecordBytes=datFileBytes[i]\n",
    "        #read the bytes for the accepted spectra\n",
    "        acceptedSpectraBytes=currentRecordBytes[acceptedSpectraDescription[\"startingByte\"]-1:acceptedSpectraDescription[\"startingByte\"]-1+acceptedSpectraDescription[\"noOfBytes\"]]\n",
    "        #read the accepted spectra for the current record\n",
    "        acceptedSpectra=[]\n",
    "        #iterate through each channel in the spectra\n",
    "        for j in range(channelsPerSpectra):\n",
    "            #get the staring index of the channel\n",
    "            startIndex=bytesPerChannel*j\n",
    "            #get the end+1 index of the channel\n",
    "            endIndex=startIndex+4\n",
    "            #get the bytes making up the current channel\n",
    "            currentChannelBytes=acceptedSpectraBytes[startIndex:endIndex]\n",
    "            #combine list into a single variable\n",
    "            currentChannelBytes=currentChannelBytes[0]+currentChannelBytes[1]+currentChannelBytes[2]+currentChannelBytes[3]\n",
    "            #convert the byte data into float 32 and append it to the spectra\n",
    "            acceptedSpectra.append(struct.unpack('f',currentChannelBytes)[0])\n",
    "\n",
    "        #read the bytes for the rejected spectra\n",
    "        rejectedSpectraBytes=currentRecordBytes[rejectedSpectraDescription[\"startingByte\"]-1:rejectedSpectraDescription[\"startingByte\"]-1+rejectedSpectraDescription[\"noOfBytes\"]]\n",
    "        #read the rejected spectra for the current record\n",
    "        rejectedSpectra=[]\n",
    "        #iterate through each channel in the spectra\n",
    "        for j in range(channelsPerSpectra):\n",
    "            #get the staring index of the channel\n",
    "            startIndex=bytesPerChannel*j\n",
    "            #get the end+1 index of the channel\n",
    "            endIndex=startIndex+4\n",
    "            #get the bytes making up the current channel\n",
    "            currentChannelBytes=rejectedSpectraBytes[startIndex:endIndex]\n",
    "            #combine list into a single variable\n",
    "            currentChannelBytes=currentChannelBytes[0]+currentChannelBytes[1]+currentChannelBytes[2]+currentChannelBytes[3]\n",
    "            #convert the byte data into float 32 and append it to the spectra\n",
    "            rejectedSpectra.append(struct.unpack('f',currentChannelBytes)[0])\n",
    "\n",
    "        #read the alltitude\n",
    "        altitudeBytes=currentRecordBytes[startingBytePerColumn[0]-1:startingBytePerColumn[0]-1+bytesPerItermPerColumn[0]]\n",
    "        altitudeBytes=altitudeBytes[0]+altitudeBytes[1]+altitudeBytes[2]+altitudeBytes[3]\n",
    "        altitude=struct.unpack('f',altitudeBytes)[0]\n",
    "        #read the latitude\n",
    "        latitudeBytes=currentRecordBytes[startingBytePerColumn[1]-1:startingBytePerColumn[1]-1+bytesPerItermPerColumn[1]]\n",
    "        latitudeBytes=latitudeBytes[0]+latitudeBytes[1]+latitudeBytes[2]+latitudeBytes[3]\n",
    "        latitude=struct.unpack('f',latitudeBytes)[0]\n",
    "        #read the longitude\n",
    "        longitudeBytes=currentRecordBytes[startingBytePerColumn[2]-1:startingBytePerColumn[2]-1+bytesPerItermPerColumn[2]]\n",
    "        longitudeBytes=longitudeBytes[0]+longitudeBytes[1]+longitudeBytes[2]+longitudeBytes[3]\n",
    "        longitude=struct.unpack('f',longitudeBytes)[0]\n",
    "\n",
    "        #append all five to their respective lists\n",
    "        #accepted spectra\n",
    "        acceptedSpectras.append(np.array(acceptedSpectra))\n",
    "        #rejected spectra\n",
    "        rejectedSpectras.append(np.array(rejectedSpectra))\n",
    "        #altitudes\n",
    "        altitudes.append(altitude)\n",
    "        #latitudes\n",
    "        latitudes.append(latitude)\n",
    "        #longitudes\n",
    "        longitudes.append(longitude)\n",
    "        \n",
    "    #create a dictionary out of the lists\n",
    "    dataDictionary={'Accepted Spectrum':acceptedSpectras,\n",
    "                    'Altitude':altitudes,\n",
    "                    'Latitude':latitudes,\n",
    "                    'Longitude':longitudes}\n",
    "    \n",
    "    #create a dataframe from the dictionary and return it\n",
    "    return pd.DataFrame(dataDictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f3099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yearAndDayKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5c14de",
   "metadata": {},
   "outputs": [],
   "source": [
    "datFileAddress=saveFile(datFileAddresses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8677b471",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmlFileAddress=saveFile(xmlFileAddresses[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c72cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the the data from .dat file into a dataframe \n",
    "currentDataFrame=readGRSData(datFileAddress,xmlFileAddress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462938f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyplot from matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14602cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creaete a numpy array containing the band numbers of the gamma ray spectrometer\n",
    "bandNumbers=np.arange(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130baee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and format the figure\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"Bands\",fontsize=24)\n",
    "plt.title(\"Spectra collected on day \"+xmlFileAddress[xmlFileAddress.index('_')+1:xmlFileAddress.rindex('_')]+\" of \"+xmlFileAddress[xmlFileAddress.rindex('/')+1:xmlFileAddress.index('_')],fontsize=30)\n",
    "\n",
    "#iterate over the dataframe\n",
    "for index,row in currentDataFrame.iterrows():\n",
    "    #get the spectra\n",
    "    currentSpectra=row['Accepted Spectrum']\n",
    "    #get the altitude\n",
    "    altitude=row['Altitude']\n",
    "    #get the latitutude and longitude\n",
    "    latitude=row['Latitude']\n",
    "    longitude=row['Longitude']\n",
    "    \n",
    "    #plot the spectra\n",
    "    plt.plot(bandNumbers[:],currentSpectra[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896ebe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create and format the figure\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.xticks(fontsize=20)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.xlabel(\"Bands\",fontsize=24)\n",
    "plt.title(\"Spectra collected on day \"+xmlFileAddress[xmlFileAddress.index('_')+1:xmlFileAddress.rindex('_')]+\" of \"+xmlFileAddress[xmlFileAddress.rindex('/')+1:xmlFileAddress.index('_')]+\" with first band removed\",fontsize=30)\n",
    "\n",
    "#iterate over the dataframe\n",
    "for index,row in currentDataFrame.iterrows():\n",
    "    #get the spectra\n",
    "    currentSpectra=row['Accepted Spectrum']\n",
    "    #get the altitude\n",
    "    altitude=row['Altitude']\n",
    "    #get the latitutude and longitude\n",
    "    latitude=row['Latitude']\n",
    "    longitude=row['Longitude']\n",
    "    \n",
    "    #plot the spectra\n",
    "    plt.plot(bandNumbers[1:],currentSpectra[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3082b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the used files from the disk\n",
    "import os\n",
    "os.remove(xmlFileAddress)\n",
    "os.remove(datFileAddress)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
