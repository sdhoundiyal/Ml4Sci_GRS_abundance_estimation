{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83832900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#expand cell width to 100%\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "276ccc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas \n",
    "import pandas as pd\n",
    "#import numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3400b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the address of the dataframe containing the pre-processed dataset\n",
    "dataFramePickleAddress=\"C:/ML4Sci/Ml4Sci_GRS_abundance_estimation/Dataset/GRSFiveDegreeSectionPreProcessedDataset.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3b151d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataframe\n",
    "dataframe=pd.read_pickle(dataFramePickleAddress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "37ac4239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Max Lat per section</th>\n",
       "      <th>Min Lat per section</th>\n",
       "      <th>Max Lon per section</th>\n",
       "      <th>Min Lon per section</th>\n",
       "      <th>No. of spectra per section</th>\n",
       "      <th>Total active time per section</th>\n",
       "      <th>Summed Spectra</th>\n",
       "      <th>Normalized Spectra</th>\n",
       "      <th>Log Normalized Spectra</th>\n",
       "      <th>Aluminum</th>\n",
       "      <th>...</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Oxygen</th>\n",
       "      <th>Potassium</th>\n",
       "      <th>Silicon</th>\n",
       "      <th>Thorium</th>\n",
       "      <th>Titanium</th>\n",
       "      <th>Uranium</th>\n",
       "      <th>Denoised Log Scaled Spectra</th>\n",
       "      <th>Continuum Removed Denoised Log Scaled Spectra</th>\n",
       "      <th>Normalized Continuum Removed Denoised Log Scaled Spectra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-87.5</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>7002</td>\n",
       "      <td>215153.707937</td>\n",
       "      <td>[270112.3731329723, 44221.71628033137, 45327.9...</td>\n",
       "      <td>[75.32634479500912, 12.332127585726989, 12.640...</td>\n",
       "      <td>[1.8769468938381042, 1.0910380092016139, 1.101...</td>\n",
       "      <td>15.151</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5168</td>\n",
       "      <td>45.289</td>\n",
       "      <td>299.99</td>\n",
       "      <td>20.793</td>\n",
       "      <td>0.46453</td>\n",
       "      <td>0.17034</td>\n",
       "      <td>0.17684</td>\n",
       "      <td>[2.158214838616408, 2.157544331369737, 2.15556...</td>\n",
       "      <td>[0.0, 0.0031096864344261066, 0.005679290051129...</td>\n",
       "      <td>[0.0, 0.008732596361173385, 0.0159485365101445...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-82.5</td>\n",
       "      <td>-87.5</td>\n",
       "      <td>-135.0</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>1599</td>\n",
       "      <td>49113.391313</td>\n",
       "      <td>[101728.76450092324, 11353.802617173642, 10565...</td>\n",
       "      <td>[124.27824075904886, 13.87051756806718, 12.906...</td>\n",
       "      <td>[2.0943950968999223, 1.1420926667518603, 1.110...</td>\n",
       "      <td>13.394</td>\n",
       "      <td>...</td>\n",
       "      <td>4.2240</td>\n",
       "      <td>45.529</td>\n",
       "      <td>376.77</td>\n",
       "      <td>22.029</td>\n",
       "      <td>0.65727</td>\n",
       "      <td>0.11806</td>\n",
       "      <td>0.14017</td>\n",
       "      <td>[2.166600464940364, 2.1658480356144825, 2.1637...</td>\n",
       "      <td>[0.0, 0.0031162181188209015, 0.005685493606940...</td>\n",
       "      <td>[0.0, 0.00854362748004175, 0.01558772125881830...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-82.5</td>\n",
       "      <td>-87.5</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>-135.0</td>\n",
       "      <td>1940</td>\n",
       "      <td>59600.854365</td>\n",
       "      <td>[69004.10988058499, 13041.311258502305, 12455....</td>\n",
       "      <td>[69.4662289143658, 13.12864863838364, 12.53878...</td>\n",
       "      <td>[1.8417737231418783, 1.118220025472807, 1.0982...</td>\n",
       "      <td>13.634</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5352</td>\n",
       "      <td>44.660</td>\n",
       "      <td>375.20</td>\n",
       "      <td>20.907</td>\n",
       "      <td>0.59783</td>\n",
       "      <td>0.16417</td>\n",
       "      <td>0.25209</td>\n",
       "      <td>[2.1624604406884194, 2.1617557577637347, 2.159...</td>\n",
       "      <td>[0.0, 0.003128077634203974, 0.0057268713902404...</td>\n",
       "      <td>[0.0, 0.008671523159558032, 0.0158757881675521...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-82.5</td>\n",
       "      <td>-87.5</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>1818</td>\n",
       "      <td>55864.552890</td>\n",
       "      <td>[51455.21453525051, 11348.088811988011, 11487....</td>\n",
       "      <td>[55.264254565457236, 12.188146033430378, 12.33...</td>\n",
       "      <td>[1.742444316396172, 1.085937649122431, 1.09122...</td>\n",
       "      <td>14.642</td>\n",
       "      <td>...</td>\n",
       "      <td>3.6490</td>\n",
       "      <td>44.706</td>\n",
       "      <td>255.60</td>\n",
       "      <td>20.687</td>\n",
       "      <td>0.44024</td>\n",
       "      <td>0.21359</td>\n",
       "      <td>0.15456</td>\n",
       "      <td>[2.156835106449848, 2.156137776072336, 2.15427...</td>\n",
       "      <td>[0.0, 0.0031223821127568385, 0.005761526562635...</td>\n",
       "      <td>[0.0, 0.008548415452029046, 0.0157738293766423...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-82.5</td>\n",
       "      <td>-87.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>1855</td>\n",
       "      <td>57007.157885</td>\n",
       "      <td>[51138.233213284286, 10203.879826977849, 12005...</td>\n",
       "      <td>[53.82296024948041, 10.739577490433051, 12.635...</td>\n",
       "      <td>[1.7309675801735431, 1.030987195964759, 1.1015...</td>\n",
       "      <td>14.030</td>\n",
       "      <td>...</td>\n",
       "      <td>3.4253</td>\n",
       "      <td>44.777</td>\n",
       "      <td>299.84</td>\n",
       "      <td>20.930</td>\n",
       "      <td>0.41695</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.17853</td>\n",
       "      <td>[2.1550525786763455, 2.15430812063849, 2.15245...</td>\n",
       "      <td>[0.0, 0.0030380779680346803, 0.005615903444584...</td>\n",
       "      <td>[0.0, 0.00860571082569534, 0.01590770269151066...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>87.5</td>\n",
       "      <td>82.5</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1748</td>\n",
       "      <td>53653.126808</td>\n",
       "      <td>[172204.68795185376, 12837.986652251333, 11392...</td>\n",
       "      <td>[192.57556626933405, 14.356650673799725, 12.73...</td>\n",
       "      <td>[2.2846011835800297, 1.1570531332525222, 1.105...</td>\n",
       "      <td>14.908</td>\n",
       "      <td>...</td>\n",
       "      <td>4.8716</td>\n",
       "      <td>44.940</td>\n",
       "      <td>935.92</td>\n",
       "      <td>20.180</td>\n",
       "      <td>1.65770</td>\n",
       "      <td>0.11112</td>\n",
       "      <td>0.47992</td>\n",
       "      <td>[2.15549824869262, 2.1548999596086973, 2.15302...</td>\n",
       "      <td>[0.0, 0.0031042509343501834, 0.005683791947784...</td>\n",
       "      <td>[0.0, 0.008672476703310141, 0.0158790491800041...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>87.5</td>\n",
       "      <td>82.5</td>\n",
       "      <td>90.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>2035</td>\n",
       "      <td>62483.348769</td>\n",
       "      <td>[69631.4865040512, 13027.290796013549, 12597.1...</td>\n",
       "      <td>[66.8640409410214, 12.509531949810704, 12.0964...</td>\n",
       "      <td>[1.8251926197192418, 1.0972410606592737, 1.082...</td>\n",
       "      <td>14.201</td>\n",
       "      <td>...</td>\n",
       "      <td>4.9509</td>\n",
       "      <td>45.074</td>\n",
       "      <td>725.37</td>\n",
       "      <td>20.989</td>\n",
       "      <td>1.47520</td>\n",
       "      <td>0.23089</td>\n",
       "      <td>0.31377</td>\n",
       "      <td>[2.14741575183071, 2.14686561246586, 2.1451899...</td>\n",
       "      <td>[0.0, 0.003080477682960492, 0.0056969814199795...</td>\n",
       "      <td>[0.0, 0.008773223933367814, 0.0162250465303427...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>87.5</td>\n",
       "      <td>82.5</td>\n",
       "      <td>135.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1903</td>\n",
       "      <td>58474.988092</td>\n",
       "      <td>[58870.11519813659, 11691.651373354718, 11794....</td>\n",
       "      <td>[60.40543191435932, 11.996566485773727, 12.102...</td>\n",
       "      <td>[1.781075993991352, 1.07905696524328, 1.082869...</td>\n",
       "      <td>14.718</td>\n",
       "      <td>...</td>\n",
       "      <td>3.5729</td>\n",
       "      <td>44.902</td>\n",
       "      <td>615.29</td>\n",
       "      <td>20.763</td>\n",
       "      <td>1.13880</td>\n",
       "      <td>0.14168</td>\n",
       "      <td>0.36836</td>\n",
       "      <td>[2.1464838830391613, 2.1458006278534687, 2.143...</td>\n",
       "      <td>[0.0, 0.003018919479894744, 0.0055261654685280...</td>\n",
       "      <td>[0.0, 0.008498570047967925, 0.0155567263862845...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>87.5</td>\n",
       "      <td>82.5</td>\n",
       "      <td>180.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>1902</td>\n",
       "      <td>58399.244640</td>\n",
       "      <td>[69077.49428235987, 10543.676356979646, 11999....</td>\n",
       "      <td>[70.97094632772614, 10.832684314943478, 12.328...</td>\n",
       "      <td>[1.8510805961570183, 1.0347360871623914, 1.090...</td>\n",
       "      <td>15.137</td>\n",
       "      <td>...</td>\n",
       "      <td>3.4631</td>\n",
       "      <td>45.209</td>\n",
       "      <td>534.21</td>\n",
       "      <td>20.913</td>\n",
       "      <td>0.92528</td>\n",
       "      <td>0.14485</td>\n",
       "      <td>0.24324</td>\n",
       "      <td>[2.1390119880861276, 2.138304773247351, 2.1364...</td>\n",
       "      <td>[0.0, 0.0029927682042373283, 0.005525986008764...</td>\n",
       "      <td>[0.0, 0.008238164470285638, 0.0152113289416292...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>90.0</td>\n",
       "      <td>87.5</td>\n",
       "      <td>180.0</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>7294</td>\n",
       "      <td>223993.416783</td>\n",
       "      <td>[342422.3644296753, 45978.50919445278, 45791.6...</td>\n",
       "      <td>[91.72297186611817, 12.316034066037092, 12.265...</td>\n",
       "      <td>[1.962478117625136, 1.0904708814873694, 1.0887...</td>\n",
       "      <td>15.185</td>\n",
       "      <td>...</td>\n",
       "      <td>4.2786</td>\n",
       "      <td>44.799</td>\n",
       "      <td>714.98</td>\n",
       "      <td>20.098</td>\n",
       "      <td>1.31050</td>\n",
       "      <td>0.14671</td>\n",
       "      <td>0.41704</td>\n",
       "      <td>[2.1479502451595986, 2.147254910678402, 2.1453...</td>\n",
       "      <td>[0.0, 0.003031719365600205, 0.0055773123862935...</td>\n",
       "      <td>[0.0, 0.008583538074052393, 0.0157907337208821...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1790 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Max Lat per section  Min Lat per section  Max Lon per section  \\\n",
       "0                   -87.5                -90.0                180.0   \n",
       "1                   -82.5                -87.5               -135.0   \n",
       "2                   -82.5                -87.5                -90.0   \n",
       "3                   -82.5                -87.5                -45.0   \n",
       "4                   -82.5                -87.5                  0.0   \n",
       "...                   ...                  ...                  ...   \n",
       "1785                 87.5                 82.5                 45.0   \n",
       "1786                 87.5                 82.5                 90.0   \n",
       "1787                 87.5                 82.5                135.0   \n",
       "1788                 87.5                 82.5                180.0   \n",
       "1789                 90.0                 87.5                180.0   \n",
       "\n",
       "      Min Lon per section  No. of spectra per section  \\\n",
       "0                  -180.0                        7002   \n",
       "1                  -180.0                        1599   \n",
       "2                  -135.0                        1940   \n",
       "3                   -90.0                        1818   \n",
       "4                   -45.0                        1855   \n",
       "...                   ...                         ...   \n",
       "1785                  0.0                        1748   \n",
       "1786                 45.0                        2035   \n",
       "1787                 90.0                        1903   \n",
       "1788                135.0                        1902   \n",
       "1789               -180.0                        7294   \n",
       "\n",
       "      Total active time per section  \\\n",
       "0                     215153.707937   \n",
       "1                      49113.391313   \n",
       "2                      59600.854365   \n",
       "3                      55864.552890   \n",
       "4                      57007.157885   \n",
       "...                             ...   \n",
       "1785                   53653.126808   \n",
       "1786                   62483.348769   \n",
       "1787                   58474.988092   \n",
       "1788                   58399.244640   \n",
       "1789                  223993.416783   \n",
       "\n",
       "                                         Summed Spectra  \\\n",
       "0     [270112.3731329723, 44221.71628033137, 45327.9...   \n",
       "1     [101728.76450092324, 11353.802617173642, 10565...   \n",
       "2     [69004.10988058499, 13041.311258502305, 12455....   \n",
       "3     [51455.21453525051, 11348.088811988011, 11487....   \n",
       "4     [51138.233213284286, 10203.879826977849, 12005...   \n",
       "...                                                 ...   \n",
       "1785  [172204.68795185376, 12837.986652251333, 11392...   \n",
       "1786  [69631.4865040512, 13027.290796013549, 12597.1...   \n",
       "1787  [58870.11519813659, 11691.651373354718, 11794....   \n",
       "1788  [69077.49428235987, 10543.676356979646, 11999....   \n",
       "1789  [342422.3644296753, 45978.50919445278, 45791.6...   \n",
       "\n",
       "                                     Normalized Spectra  \\\n",
       "0     [75.32634479500912, 12.332127585726989, 12.640...   \n",
       "1     [124.27824075904886, 13.87051756806718, 12.906...   \n",
       "2     [69.4662289143658, 13.12864863838364, 12.53878...   \n",
       "3     [55.264254565457236, 12.188146033430378, 12.33...   \n",
       "4     [53.82296024948041, 10.739577490433051, 12.635...   \n",
       "...                                                 ...   \n",
       "1785  [192.57556626933405, 14.356650673799725, 12.73...   \n",
       "1786  [66.8640409410214, 12.509531949810704, 12.0964...   \n",
       "1787  [60.40543191435932, 11.996566485773727, 12.102...   \n",
       "1788  [70.97094632772614, 10.832684314943478, 12.328...   \n",
       "1789  [91.72297186611817, 12.316034066037092, 12.265...   \n",
       "\n",
       "                                 Log Normalized Spectra  Aluminum  ...  \\\n",
       "0     [1.8769468938381042, 1.0910380092016139, 1.101...    15.151  ...   \n",
       "1     [2.0943950968999223, 1.1420926667518603, 1.110...    13.394  ...   \n",
       "2     [1.8417737231418783, 1.118220025472807, 1.0982...    13.634  ...   \n",
       "3     [1.742444316396172, 1.085937649122431, 1.09122...    14.642  ...   \n",
       "4     [1.7309675801735431, 1.030987195964759, 1.1015...    14.030  ...   \n",
       "...                                                 ...       ...  ...   \n",
       "1785  [2.2846011835800297, 1.1570531332525222, 1.105...    14.908  ...   \n",
       "1786  [1.8251926197192418, 1.0972410606592737, 1.082...    14.201  ...   \n",
       "1787  [1.781075993991352, 1.07905696524328, 1.082869...    14.718  ...   \n",
       "1788  [1.8510805961570183, 1.0347360871623914, 1.090...    15.137  ...   \n",
       "1789  [1.962478117625136, 1.0904708814873694, 1.0887...    15.185  ...   \n",
       "\n",
       "      Magnesium  Oxygen  Potassium  Silicon  Thorium  Titanium  Uranium  \\\n",
       "0        3.5168  45.289     299.99   20.793  0.46453   0.17034  0.17684   \n",
       "1        4.2240  45.529     376.77   22.029  0.65727   0.11806  0.14017   \n",
       "2        3.5352  44.660     375.20   20.907  0.59783   0.16417  0.25209   \n",
       "3        3.6490  44.706     255.60   20.687  0.44024   0.21359  0.15456   \n",
       "4        3.4253  44.777     299.84   20.930  0.41695   0.00000  0.17853   \n",
       "...         ...     ...        ...      ...      ...       ...      ...   \n",
       "1785     4.8716  44.940     935.92   20.180  1.65770   0.11112  0.47992   \n",
       "1786     4.9509  45.074     725.37   20.989  1.47520   0.23089  0.31377   \n",
       "1787     3.5729  44.902     615.29   20.763  1.13880   0.14168  0.36836   \n",
       "1788     3.4631  45.209     534.21   20.913  0.92528   0.14485  0.24324   \n",
       "1789     4.2786  44.799     714.98   20.098  1.31050   0.14671  0.41704   \n",
       "\n",
       "                            Denoised Log Scaled Spectra  \\\n",
       "0     [2.158214838616408, 2.157544331369737, 2.15556...   \n",
       "1     [2.166600464940364, 2.1658480356144825, 2.1637...   \n",
       "2     [2.1624604406884194, 2.1617557577637347, 2.159...   \n",
       "3     [2.156835106449848, 2.156137776072336, 2.15427...   \n",
       "4     [2.1550525786763455, 2.15430812063849, 2.15245...   \n",
       "...                                                 ...   \n",
       "1785  [2.15549824869262, 2.1548999596086973, 2.15302...   \n",
       "1786  [2.14741575183071, 2.14686561246586, 2.1451899...   \n",
       "1787  [2.1464838830391613, 2.1458006278534687, 2.143...   \n",
       "1788  [2.1390119880861276, 2.138304773247351, 2.1364...   \n",
       "1789  [2.1479502451595986, 2.147254910678402, 2.1453...   \n",
       "\n",
       "          Continuum Removed Denoised Log Scaled Spectra  \\\n",
       "0     [0.0, 0.0031096864344261066, 0.005679290051129...   \n",
       "1     [0.0, 0.0031162181188209015, 0.005685493606940...   \n",
       "2     [0.0, 0.003128077634203974, 0.0057268713902404...   \n",
       "3     [0.0, 0.0031223821127568385, 0.005761526562635...   \n",
       "4     [0.0, 0.0030380779680346803, 0.005615903444584...   \n",
       "...                                                 ...   \n",
       "1785  [0.0, 0.0031042509343501834, 0.005683791947784...   \n",
       "1786  [0.0, 0.003080477682960492, 0.0056969814199795...   \n",
       "1787  [0.0, 0.003018919479894744, 0.0055261654685280...   \n",
       "1788  [0.0, 0.0029927682042373283, 0.005525986008764...   \n",
       "1789  [0.0, 0.003031719365600205, 0.0055773123862935...   \n",
       "\n",
       "     Normalized Continuum Removed Denoised Log Scaled Spectra  \n",
       "0     [0.0, 0.008732596361173385, 0.0159485365101445...        \n",
       "1     [0.0, 0.00854362748004175, 0.01558772125881830...        \n",
       "2     [0.0, 0.008671523159558032, 0.0158757881675521...        \n",
       "3     [0.0, 0.008548415452029046, 0.0157738293766423...        \n",
       "4     [0.0, 0.00860571082569534, 0.01590770269151066...        \n",
       "...                                                 ...        \n",
       "1785  [0.0, 0.008672476703310141, 0.0158790491800041...        \n",
       "1786  [0.0, 0.008773223933367814, 0.0162250465303427...        \n",
       "1787  [0.0, 0.008498570047967925, 0.0155567263862845...        \n",
       "1788  [0.0, 0.008238164470285638, 0.0152113289416292...        \n",
       "1789  [0.0, 0.008583538074052393, 0.0157907337208821...        \n",
       "\n",
       "[1790 rows x 22 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print the dataframe\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "79629eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a variable epsilon, which contains a very small value that is added to all inputs and outputs to make sure no values are 0\n",
    "epsilon=1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e00ab13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the abundance values for the seven elements for which the model is to be trained\n",
    "#aluminium\n",
    "aluminiumAbundances=dataframe['Aluminum'].to_numpy()+epsilon\n",
    "#calcium\n",
    "calciumAbundances=dataframe['Calcium'].to_numpy()+epsilon\n",
    "#iron\n",
    "ironAbundances=dataframe['Iron'].to_numpy()+epsilon\n",
    "#magnesium\n",
    "magnesiumAbundances=dataframe['Magnesium'].to_numpy()+epsilon\n",
    "#oxygen\n",
    "oxygenAbundances=dataframe['Oxygen'].to_numpy()+epsilon\n",
    "#silicon\n",
    "siliconAbundances=dataframe['Silicon'].to_numpy()+epsilon\n",
    "#titanium\n",
    "titaniumAbundances=dataframe['Titanium'].to_numpy()+epsilon\n",
    "#potassium\n",
    "potassiumAbundances=dataframe['Potassium'].to_numpy()+epsilon\n",
    "#uranium\n",
    "uraniumAbundances=dataframe['Uranium'].to_numpy()+epsilon\n",
    "#thorium\n",
    "thoriumAbundances=dataframe['Thorium'].to_numpy()+epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f78ae584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an list which contains the element names to be estimated\n",
    "elementNames=['Aluminum',\n",
    "              'Calcium',\n",
    "              'Iron',\n",
    "              'Magnesium',\n",
    "              'Oxygen',\n",
    "              'Silicon',\n",
    "              'Titanium',\n",
    "              'Potassium',\n",
    "              'Uranium',\n",
    "              'Thorium']\n",
    "#conver the list to a numpy array\n",
    "elementNames=np.array(elementNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f7f5325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the elemental abundances into a single matrix\n",
    "allElementalAbundances=np.dstack([aluminiumAbundances,\n",
    "                                  calciumAbundances,\n",
    "                                  ironAbundances,\n",
    "                                  magnesiumAbundances,\n",
    "                                  oxygenAbundances,\n",
    "                                  siliconAbundances,\n",
    "                                  titaniumAbundances])\n",
    "#reshape the abundance matrix\n",
    "allElementalAbundances=allElementalAbundances[0,:,:]\n",
    "#rescale weigth percent values from % (0-100) to franctions (0-1)\n",
    "allElementalAbundances=allElementalAbundances/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93e0a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale the atomic mineral values between 0 and 1\n",
    "potassiumAbundances=(potassiumAbundances-np.amin(potassiumAbundances))/np.ptp(potassiumAbundances)\n",
    "uraniumAbundances=(uraniumAbundances-np.amin(uraniumAbundances))/np.ptp(uraniumAbundances)\n",
    "thoriumAbundances=(thoriumAbundances-np.amin(thoriumAbundances))/np.ptp(thoriumAbundances)\n",
    "#combine atomic abundances into a single array\n",
    "atomicAbundances=np.dstack([potassiumAbundances,uraniumAbundances,thoriumAbundances])[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "649e38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine all abundances into a single array\n",
    "allElementalAbundances=np.append(allElementalAbundances,\n",
    "                                 atomicAbundances,\n",
    "                                 axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0e32348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the pre-processed spectra as a numpy array\n",
    "preprocessedSpectra=dataframe['Normalized Continuum Removed Denoised Log Scaled Spectra'].to_numpy()\n",
    "#reshape the numpy array\n",
    "preprocessedSpectra=np.vstack(preprocessedSpectra)+epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc6d40e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the eneergy values for each band\n",
    "gain=17.8 #keV/channel\n",
    "energyBands=np.arange(0,512,1)*gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa371af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the index of the last relavent band\n",
    "finalRelaventBandIndex=np.argmin(np.abs(energyBands-8000))\n",
    "#compute the index of the first relavent band\n",
    "firstRelaventBandIndex=finalRelaventBandIndex-preprocessedSpectra.shape[1]+1\n",
    "#get the energies of the relavent bands\n",
    "relaventEnergyBands=energyBands[firstRelaventBandIndex:finalRelaventBandIndex+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4d5cf87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the variables no longer needed from memory\n",
    "del firstRelaventBandIndex\n",
    "del finalRelaventBandIndex\n",
    "del energyBands\n",
    "del gain\n",
    "del dataframe\n",
    "del dataFramePickleAddress\n",
    "del aluminiumAbundances\n",
    "del calciumAbundances\n",
    "del ironAbundances\n",
    "del magnesiumAbundances\n",
    "del oxygenAbundances\n",
    "del siliconAbundances\n",
    "del titaniumAbundances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6aabc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyplot from matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0badb81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set plot parameters\n",
    "baseFontSize=18\n",
    "noOfBinsForHistogram=100\n",
    "noOfXticks=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31fb567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure \n",
    "figure,axes=plt.subplots(nrows=1,\n",
    "                         ncols=10,\n",
    "                         figsize=(50,5),\n",
    "                         dpi=200)\n",
    "#iterate over all elements\n",
    "for i in range(elementNames.shape[0]):\n",
    "    #plot the histogram\n",
    "    temp=axes[i].hist(allElementalAbundances[:,i],\n",
    "                      bins=noOfBinsForHistogram)\n",
    "    #set the title of the figure\n",
    "    axes[i].set_title(elementNames[i],fontsize=baseFontSize*1.2)\n",
    "    #set the axis labels\n",
    "    axes[i].set_xlabel(\"Wt frac.\",fontsize=baseFontSize*1.2)\n",
    "    axes[i].set_ylabel(\"Freq\",fontsize=baseFontSize*1.2)\n",
    "    #set the ticks and their label sizes\n",
    "    axes[i].set_xticks(np.arange(np.amin(allElementalAbundances[:,i]),\n",
    "                                 np.amax(allElementalAbundances[:,i])+np.ptp(allElementalAbundances[:,i])/noOfXticks,\n",
    "                                 np.ptp(allElementalAbundances[:,i])/noOfXticks),\n",
    "                       labels=np.round(np.arange(np.amin(allElementalAbundances[:,i]),\n",
    "                                                 np.amax(allElementalAbundances[:,i])+np.ptp(allElementalAbundances[:,i])/noOfXticks,\n",
    "                                                 np.ptp(allElementalAbundances[:,i])/noOfXticks),\n",
    "                                       2),\n",
    "                       fontsize=baseFontSize)\n",
    "    #set the margins\n",
    "    axes[i].margins(0.01)\n",
    "    \n",
    "#add a title\n",
    "figure.suptitle(\"Distribution of abundance values\",\n",
    "                fontsize=baseFontSize*1.5)\n",
    "#adjust the layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure\n",
    "plt.figure(figsize=(30,15),\n",
    "           dpi=100)\n",
    "#and plot all the spectra\n",
    "for i in range(preprocessedSpectra.shape[0]):\n",
    "    plt.plot(relaventEnergyBands,\n",
    "             preprocessedSpectra[i,:],\n",
    "             lw=5)\n",
    "#annotate the figure\n",
    "plt.title(\"Preprocessed GRS Spectra\",\n",
    "          fontsize=baseFontSize*1.8)\n",
    "plt.xticks(np.arange(np.amin(relaventEnergyBands),\n",
    "                     np.amax(relaventEnergyBands)+np.ptp(relaventEnergyBands)/noOfXticks,\n",
    "                     np.ptp(relaventEnergyBands)/noOfXticks),\n",
    "           fontsize=baseFontSize*1.2)\n",
    "plt.yticks(np.arange(np.amin(preprocessedSpectra),\n",
    "                     np.amax(preprocessedSpectra)+np.ptp(preprocessedSpectra)/noOfXticks,\n",
    "                     np.ptp(preprocessedSpectra)/noOfXticks),\n",
    "           labels=np.round(np.arange(np.amin(preprocessedSpectra),\n",
    "                                     np.amax(preprocessedSpectra)+np.ptp(preprocessedSpectra)/noOfXticks,\n",
    "                                     np.ptp(preprocessedSpectra)/noOfXticks),\n",
    "                           2),\n",
    "           fontsize=baseFontSize*1.2)\n",
    "plt.xlabel(\"KeV\",\n",
    "           fontsize=baseFontSize*1.5)\n",
    "plt.ylabel(\"log(Counts/min) ratioed\",\n",
    "           fontsize=baseFontSize*1.5)\n",
    "plt.margins(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9187d1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfcb25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a global seed value\n",
    "globalSeed=23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481f6695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the pre-trained models\n",
    "#define a funtion to calculate the Pearsons correlation between two sets of values\n",
    "def pearson_correlation(x,y):\n",
    "    x_mean=tf.reduce_mean(x)\n",
    "    y_mean=tf.reduce_mean(y)\n",
    "    x_diff=x-x_mean\n",
    "    y_diff=y-y_mean\n",
    "    covariance=tf.reduce_mean(tf.multiply(x_diff,y_diff))\n",
    "    x_std=tf.sqrt(tf.reduce_mean(tf.square(x_diff)))\n",
    "    y_std=tf.sqrt(tf.reduce_mean(tf.square(y_diff)))\n",
    "    correlation=covariance/(x_std*y_std)\n",
    "    return correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1afb7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a funtion which creates the CAM (Channel Attention Module)\n",
    "def createCAM(inputFeatureBlock,reductionRatio):\n",
    "    #perform max pooling along the channel dimension\n",
    "    channelMaxPooledFeatures=tf.math.reduce_max(inputFeatureBlock,\n",
    "                                                axis=1,\n",
    "                                                keepdims=False)\n",
    "    \n",
    "    #perform avg pooling along the channel dimension\n",
    "    channelAvgPooledFeatures=tf.math.reduce_mean(inputFeatureBlock,\n",
    "                                                 axis=1,\n",
    "                                                 keepdims=False)\n",
    "    \n",
    "    #create the bottleneck for the MLP\n",
    "    bottleneckLayer=tf.keras.layers.Dense(channelAvgPooledFeatures.shape[-1]//reductionRatio,\n",
    "                                          activation='relu')\n",
    "    \n",
    "    #create the recontruction layer for the MLP\n",
    "    outputLayer=tf.keras.layers.Dense(channelAvgPooledFeatures.shape[-1],\n",
    "                                      activation='relu')\n",
    "    \n",
    "    #create a dropout layer for both MLP layers\n",
    "    bottleNeckDropout=tf.keras.layers.Dropout(0.5,\n",
    "                                              noise_shape=None,\n",
    "                                              seed=globalSeed)\n",
    "    \n",
    "    OutputDropout=tf.keras.layers.Dropout(0.5,\n",
    "                                          noise_shape=None,\n",
    "                                          seed=globalSeed)\n",
    "    \n",
    "    #pass the max pooled features through the bottle-neck\n",
    "    reconstructeedMaxPooledFeatures=outputLayer(bottleneckLayer(channelMaxPooledFeatures))\n",
    "    \n",
    "    \n",
    "    #pass the avg pooled features through the bottle-neck\n",
    "    reconstructeedAvgPooledFeatures=outputLayer(bottleneckLayer(channelMaxPooledFeatures))\n",
    "    \n",
    "    \n",
    "    #add the two reconstructed features together\n",
    "    summedFeatures=tf.math.add(reconstructeedMaxPooledFeatures,\n",
    "                               reconstructeedAvgPooledFeatures)\n",
    "    \n",
    "    #apply sigmoid activation to the summed features to get the channel attention map\n",
    "    channelAttentionMap=tf.keras.activations.sigmoid(summedFeatures)\n",
    "    \n",
    "    #return the channel attention map\n",
    "    return channelAttentionMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f82a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to create the Spatial attention module (SAM)\n",
    "def createSAM(inputFeatureBlock,kernelSize):\n",
    "    #perform max pooling on the input features\n",
    "    maxPooledFeatureMap=tf.math.reduce_max(inputFeatureBlock,\n",
    "                                           axis=-1,\n",
    "                                           keepdims=False)\n",
    "    #perform average pooling on the input features\n",
    "    averagePooledFeatureMap=tf.math.reduce_mean(inputFeatureBlock,\n",
    "                                                 axis=-1,\n",
    "                                                 keepdims=False)\n",
    "    \n",
    "    #concatenate the feature maps together\n",
    "    concatenatedFeatureMaps=tf.concat([tf.expand_dims(maxPooledFeatureMap,\n",
    "                                                      axis=-1),\n",
    "                                       tf.expand_dims(averagePooledFeatureMap,\n",
    "                                                      axis=-1)],\n",
    "                                      axis=-1)\n",
    "    \n",
    "    #create the convolutional layer to be applied to the concatenated feature map\n",
    "    convolutionLayer=tf.keras.layers.Conv1D(filters=1,\n",
    "                                            kernel_size=kernelSize,\n",
    "                                            strides=1,\n",
    "                                            padding='same',\n",
    "                                            activation='sigmoid')\n",
    "    #create and add a dropout layer\n",
    "    spatialDropoutLayer=tf.keras.layers.Dropout(0.5,\n",
    "                                                noise_shape=None,\n",
    "                                                seed=globalSeed)\n",
    "    \n",
    "    \n",
    "    #get the spatial attention map\n",
    "    spatialAttentionMap=convolutionLayer(concatenatedFeatureMaps)\n",
    "    \n",
    "    \n",
    "    #return the channel attention map\n",
    "    return spatialAttentionMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56abae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function which creates a CBAM block\n",
    "def createCBAM(inputLayer,convolutionalKernelSize,noOfConvolutionalFilters,reductionRatio,spatialKernelSize):\n",
    "    #create a convolutional layer\n",
    "    convolutionalLayer=tf.keras.layers.Conv1D(filters=noOfConvolutionalFilters,\n",
    "                                              kernel_size=convolutionalKernelSize,\n",
    "                                              strides=1,\n",
    "                                              padding='same')\n",
    "    #get the feature block from the convolutional layer\n",
    "    convolutionalFeatures=convolutionalLayer(inputLayer)\n",
    "    \n",
    "    #get the channel attention map\n",
    "    channelAttentionMap=createCAM(convolutionalFeatures,\n",
    "                                  reductionRatio)\n",
    "    \n",
    "    #replicate the channel attention to make it multiplicative with the features\n",
    "    replicatedChannelAttentionMaps=tf.expand_dims(channelAttentionMap,\n",
    "                                                  axis=1)\n",
    "    replicatedChannelAttentionMaps=tf.repeat(replicatedChannelAttentionMaps,\n",
    "                                             convolutionalFeatures.shape[1],\n",
    "                                             axis=1)\n",
    "    \n",
    "    #compute the channel refined feature by performing element-wise multiplication between the features and the channel attention maps\n",
    "    channelRefinedFeatures=tf.math.multiply(replicatedChannelAttentionMaps,\n",
    "                                            convolutionalFeatures)\n",
    "    \n",
    "    '''\n",
    "    #apply batch norm to the refined features\n",
    "    channelRefinedFeatures=tf.keras.layers.BatchNormalization()(channelRefinedFeatures)\n",
    "    '''\n",
    "    \n",
    "    #get the spatial attention map\n",
    "    spatialAttentionMap=createSAM(channelRefinedFeatures,\n",
    "                                  spatialKernelSize)\n",
    "    \n",
    "    #replicate the attention map to make it multiplicative with the channel-refined features\n",
    "    replicatedSpatialAttentionMaps=tf.repeat(spatialAttentionMap,\n",
    "                                             channelRefinedFeatures.shape[-1],\n",
    "                                             axis=-1)\n",
    "    \n",
    "    #multiply the attention map with the channel refined features\n",
    "    spatiallyRefinedFeatures=tf.math.multiply(replicatedSpatialAttentionMaps,\n",
    "                                              channelRefinedFeatures)\n",
    "    \n",
    "    '''\n",
    "    #apply batch norm to the refined features\n",
    "    spatiallyRefinedFeatures=tf.keras.layers.BatchNormalization()(spatiallyRefinedFeatures)\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    #add the refined features to the original features\n",
    "    refinedFeatures=tf.math.add(convolutionalFeatures,\n",
    "                                spatiallyRefinedFeatures)\n",
    "    \n",
    "    #return the refined features (i.e. the output of the CBAM)\n",
    "    return refinedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c18281",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the number of channels in the pre-processed spectra\n",
    "noOfChannels=preprocessedSpectra.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74314f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an input layer\n",
    "inputLayer=tf.keras.Input(shape=(noOfChannels,\n",
    "                                 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770fc673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a NN (Neural Network) graph containing just the input layer\n",
    "nnGraph=inputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e05acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a convolutional block\n",
    "firstConvolutionalLayer=tf.keras.layers.Conv1D(filters=64,\n",
    "                                               kernel_size=7,\n",
    "                                               strides=1,\n",
    "                                               padding='same',\n",
    "                                               name=\"1st_Conv_layer\")\n",
    "#add the 1st Conv layer to the graph\n",
    "nnGraph=firstConvolutionalLayer(nnGraph)\n",
    "\n",
    "#apply Relu activation\n",
    "firstReluActivation=tf.keras.layers.Activation('relu',\n",
    "                                               name=\"1st_Activation\")\n",
    "#add the 1st activation layer to the graph\n",
    "nnGraph=firstReluActivation(nnGraph)\n",
    "\n",
    "#apply batch normalization\n",
    "firstBatchNormalization=tf.keras.layers.BatchNormalization(name=\"1st_Batch_Norm\")\n",
    "#add the 1st batch-norm layer to the graph\n",
    "nnGraph=firstBatchNormalization(nnGraph)\n",
    "\n",
    "#apply dropout\n",
    "firstDropoutLayer=tf.keras.layers.Dropout(0.5,\n",
    "                                          noise_shape=None,\n",
    "                                          seed=globalSeed,\n",
    "                                          name=\"1st_Dropout\")\n",
    "#add the 1st dropout layer to the graph\n",
    "nnGraph=firstDropoutLayer(nnGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6426a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list containing the number of features to be outputted by each CBAM block\n",
    "noOfChannelsInEachCBAMBlock=[64,64,64,64,\n",
    "                             128,128,128,128,\n",
    "                             256,256,256,256,\n",
    "                             512,512,512,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8823f626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iterate through all the number of channels and add CBAM blocks\n",
    "for i in range(len(noOfChannelsInEachCBAMBlock)):\n",
    "    #get the current number of channels \n",
    "    currentNumberOfChannels=noOfChannelsInEachCBAMBlock[i]\n",
    "    #add a CBAM block to the network\n",
    "    nnGraph=createCBAM(nnGraph,\n",
    "                       convolutionalKernelSize=3,\n",
    "                       noOfConvolutionalFilters=currentNumberOfChannels,\n",
    "                       reductionRatio=6,spatialKernelSize=7)\n",
    "\n",
    "    #create a flag indicating if the features are to be pooled\n",
    "    performPooling=False\n",
    "    try:\n",
    "        #check if the number of of channels changes in the next block, if so set the pooling flag to true\n",
    "        if noOfChannelsInEachCBAMBlock[i]!=noOfChannelsInEachCBAMBlock[i+1]:\n",
    "            performPooling=True\n",
    "    except:\n",
    "        performPooling=False\n",
    "    #check if the flag is true, if it is perform average pooling\n",
    "    if performPooling:\n",
    "        nnGraph=tf.keras.layers.AveragePooling1D(pool_size=2)(nnGraph)\n",
    "    \n",
    "    #print the shape of the current tensor\n",
    "    print(f\"Shape of the tensor outputted by the {i}th CBAM block #{nnGraph.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4c68d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform global average pooling\n",
    "nnGraph=tf.keras.layers.GlobalAveragePooling1D()(nnGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab27982",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the shape of the average pooled features\n",
    "print(f\"No. of nodes outputted by the Global pooling layer {nnGraph.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3320bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a fully connected layer to the network (output layer)\n",
    "preWtPerecentLayer=tf.keras.layers.Dense(7,\n",
    "                                         activation='relu',\n",
    "                                         name=\"7_Element_Pre_abundance\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12c524e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function which given the output of a layer, ensures the sum of the values is one\n",
    "#it does this by computing the sum of the nodes and dividing each note by it\n",
    "def estimateAbundances(inputNodes):\n",
    "    sampleWiseSums=tf.keras.backend.sum(inputNodes,\n",
    "                                        axis=-1,\n",
    "                                        keepdims=True)\n",
    "    sampleWiseSums=tf.repeat(sampleWiseSums,\n",
    "                             sampleWiseSums.shape[-1],\n",
    "                             axis=-1)\n",
    "    return inputNodes/(sampleWiseSums+1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b97ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a normalization layer to the network\n",
    "wtPercentAbundanceValues=estimateAbundances(preWtPerecentLayer(nnGraph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0a98d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a fully connected layer to the network for estimating atomic abundances\n",
    "AtomicAbundanceLayer=tf.keras.layers.Dense(3,\n",
    "                                           activation='relu',\n",
    "                                           name=\"Atomic_Element_Abundance\")\n",
    "atomicAbundances=AtomicAbundanceLayer(nnGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6226b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the abundances of all ten elements\n",
    "abundanceEmbedding=tf.keras.layers.Concatenate(axis=-1)([wtPercentAbundanceValues,atomicAbundances])\n",
    "nnGraph=abundanceEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b9cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the shape of the output layer\n",
    "print(f\"No. of nodes in the output layer {nnGraph.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96227cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model\n",
    "jointAbundanceEstimator=tf.keras.Model(inputs=inputLayer,\n",
    "                                       outputs=[nnGraph],\n",
    "                                       name=\"Multitask_Abundance_estimator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8f77493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the loss function for the embedding, Mean Squared Error\n",
    "lossFunction=tf.keras.losses.MeanSquaredError()\n",
    "#create the optimizer\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "#set the number of epochs the model is to be trained for\n",
    "noOfEpochs=200\n",
    "#set the batch size\n",
    "batchSize=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37dff495",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "jointAbundanceEstimator.compile(optimizer=optimizer,\n",
    "                                loss=lossFunction,\n",
    "                                metrics=[pearson_correlation],\n",
    "                                steps_per_execution=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "68044041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the address where the untrained model will be saved\n",
    "untrainedModelAddress=\"C:/ML4Sci/Ml4Sci_GRS_abundance_estimation/Models/untrained_Joint_Abundance_Attention_Model.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d7645d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the untrained model\n",
    "tf.keras.models.save_model(jointAbundanceEstimator,\n",
    "                           untrainedModelAddress,\n",
    "                           overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84d79640",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import KFold from sklearn, to be used for KFold validation\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4f25c6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set no. of folds over which the model is to be validated\n",
    "noOfFolds=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11c43ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a K-Fold splitter\n",
    "splitter=KFold(n_splits=noOfFolds,\n",
    "               shuffle=True,\n",
    "               random_state=globalSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8694ad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available\n"
     ]
    }
   ],
   "source": [
    "#check if the GPU is available\n",
    "if len(tf.config.list_physical_devices('GPU'))==1:\n",
    "    print(\"GPU available\")\n",
    "else:\n",
    "    print(\"GPU unavailable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "916aadf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list to hold model training history\n",
    "modelTrainingHistories=[]\n",
    "#create a list to store the trained models\n",
    "trainedModels=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea16cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting processing fold 1\n",
      "Epoch 1/500\n",
      "45/45 [==============================] - 29s 640ms/step - loss: 0.0051 - pearson_correlation: 0.9134 - val_loss: 0.0023 - val_pearson_correlation: 0.9403\n",
      "Epoch 2/500\n",
      "45/45 [==============================] - 5s 121ms/step - loss: 9.0944e-04 - pearson_correlation: 0.9797 - val_loss: 0.0014 - val_pearson_correlation: 0.9688\n",
      "Epoch 3/500\n",
      "45/45 [==============================] - 6s 124ms/step - loss: 8.5906e-04 - pearson_correlation: 0.9807 - val_loss: 0.0012 - val_pearson_correlation: 0.9733\n",
      "Epoch 4/500\n",
      "45/45 [==============================] - 6s 144ms/step - loss: 8.3365e-04 - pearson_correlation: 0.9813 - val_loss: 0.0012 - val_pearson_correlation: 0.9739\n",
      "Epoch 5/500\n",
      "45/45 [==============================] - 6s 125ms/step - loss: 8.1619e-04 - pearson_correlation: 0.9819 - val_loss: 0.0011 - val_pearson_correlation: 0.9761\n",
      "Epoch 6/500\n",
      "45/45 [==============================] - 6s 122ms/step - loss: 7.6391e-04 - pearson_correlation: 0.9830 - val_loss: 0.0011 - val_pearson_correlation: 0.9776\n",
      "Epoch 7/500\n",
      "45/45 [==============================] - 6s 134ms/step - loss: 3.2210e-04 - pearson_correlation: 0.9916 - val_loss: 5.6837e-04 - val_pearson_correlation: 0.9869\n",
      "Epoch 8/500\n",
      "45/45 [==============================] - 6s 122ms/step - loss: 2.6426e-04 - pearson_correlation: 0.9931 - val_loss: 4.9825e-04 - val_pearson_correlation: 0.9878\n",
      "Epoch 9/500\n",
      "45/45 [==============================] - 6s 139ms/step - loss: 2.6679e-04 - pearson_correlation: 0.9931 - val_loss: 4.8409e-04 - val_pearson_correlation: 0.9875\n",
      "Epoch 10/500\n",
      "45/45 [==============================] - 6s 130ms/step - loss: 2.3874e-04 - pearson_correlation: 0.9938 - val_loss: 4.5714e-04 - val_pearson_correlation: 0.9880\n",
      "Epoch 11/500\n",
      "45/45 [==============================] - 6s 125ms/step - loss: 2.1815e-04 - pearson_correlation: 0.9943 - val_loss: 4.5857e-04 - val_pearson_correlation: 0.9878\n",
      "Epoch 12/500\n",
      "45/45 [==============================] - 6s 132ms/step - loss: 2.0706e-04 - pearson_correlation: 0.9946 - val_loss: 4.2005e-04 - val_pearson_correlation: 0.9894\n",
      "Epoch 13/500\n",
      "45/45 [==============================] - 6s 131ms/step - loss: 2.0693e-04 - pearson_correlation: 0.9946 - val_loss: 3.1220e-04 - val_pearson_correlation: 0.9921\n",
      "Epoch 14/500\n",
      "45/45 [==============================] - 8s 173ms/step - loss: 1.9861e-04 - pearson_correlation: 0.9948 - val_loss: 2.3004e-04 - val_pearson_correlation: 0.9941\n",
      "Epoch 15/500\n",
      "45/45 [==============================] - 9s 196ms/step - loss: 1.9327e-04 - pearson_correlation: 0.9949 - val_loss: 2.4767e-04 - val_pearson_correlation: 0.9937\n",
      "Epoch 16/500\n",
      "45/45 [==============================] - 6s 130ms/step - loss: 1.9713e-04 - pearson_correlation: 0.9949 - val_loss: 1.8997e-04 - val_pearson_correlation: 0.9951\n",
      "Epoch 17/500\n",
      "45/45 [==============================] - 6s 127ms/step - loss: 1.8836e-04 - pearson_correlation: 0.9951 - val_loss: 1.9092e-04 - val_pearson_correlation: 0.9952\n",
      "Epoch 18/500\n",
      "45/45 [==============================] - 6s 125ms/step - loss: 1.8309e-04 - pearson_correlation: 0.9952 - val_loss: 1.9674e-04 - val_pearson_correlation: 0.9948\n",
      "Epoch 19/500\n",
      "45/45 [==============================] - 6s 124ms/step - loss: 1.8173e-04 - pearson_correlation: 0.9953 - val_loss: 1.8157e-04 - val_pearson_correlation: 0.9953\n",
      "Epoch 20/500\n",
      "45/45 [==============================] - 6s 125ms/step - loss: 1.8207e-04 - pearson_correlation: 0.9952 - val_loss: 1.7884e-04 - val_pearson_correlation: 0.9954\n",
      "Epoch 21/500\n",
      "45/45 [==============================] - 6s 124ms/step - loss: 1.7474e-04 - pearson_correlation: 0.9954 - val_loss: 1.7017e-04 - val_pearson_correlation: 0.9956\n",
      "Epoch 22/500\n",
      "45/45 [==============================] - 6s 127ms/step - loss: 1.7528e-04 - pearson_correlation: 0.9954 - val_loss: 1.6674e-04 - val_pearson_correlation: 0.9957\n",
      "Epoch 23/500\n",
      "45/45 [==============================] - 6s 125ms/step - loss: 1.6900e-04 - pearson_correlation: 0.9956 - val_loss: 1.9000e-04 - val_pearson_correlation: 0.9951\n",
      "Epoch 24/500\n",
      "45/45 [==============================] - 6s 127ms/step - loss: 1.7121e-04 - pearson_correlation: 0.9955 - val_loss: 1.6731e-04 - val_pearson_correlation: 0.9957\n",
      "Epoch 25/500\n",
      "45/45 [==============================] - 6s 127ms/step - loss: 1.7258e-04 - pearson_correlation: 0.9955 - val_loss: 1.6419e-04 - val_pearson_correlation: 0.9958\n",
      "Epoch 26/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.6951e-04 - pearson_correlation: 0.9956 - val_loss: 1.9605e-04 - val_pearson_correlation: 0.9950\n",
      "Epoch 27/500\n",
      "45/45 [==============================] - 6s 126ms/step - loss: 1.6873e-04 - pearson_correlation: 0.9956 - val_loss: 1.7647e-04 - val_pearson_correlation: 0.9955\n",
      "Epoch 28/500\n",
      "45/45 [==============================] - 6s 126ms/step - loss: 1.6788e-04 - pearson_correlation: 0.9956 - val_loss: 1.7138e-04 - val_pearson_correlation: 0.9957\n",
      "Epoch 29/500\n",
      "45/45 [==============================] - 6s 126ms/step - loss: 1.6036e-04 - pearson_correlation: 0.9958 - val_loss: 1.7542e-04 - val_pearson_correlation: 0.9956\n",
      "Epoch 30/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.6777e-04 - pearson_correlation: 0.9956 - val_loss: 1.6677e-04 - val_pearson_correlation: 0.9958\n",
      "Epoch 31/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.6487e-04 - pearson_correlation: 0.9957 - val_loss: 1.6099e-04 - val_pearson_correlation: 0.9960\n",
      "Epoch 32/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.6102e-04 - pearson_correlation: 0.9958 - val_loss: 1.5412e-04 - val_pearson_correlation: 0.9961\n",
      "Epoch 33/500\n",
      "45/45 [==============================] - 6s 126ms/step - loss: 1.6249e-04 - pearson_correlation: 0.9957 - val_loss: 1.6113e-04 - val_pearson_correlation: 0.9960\n",
      "Epoch 34/500\n",
      "45/45 [==============================] - 6s 130ms/step - loss: 1.7244e-04 - pearson_correlation: 0.9955 - val_loss: 1.6322e-04 - val_pearson_correlation: 0.9958\n",
      "Epoch 35/500\n",
      "45/45 [==============================] - 6s 131ms/step - loss: 1.5830e-04 - pearson_correlation: 0.9958 - val_loss: 1.6964e-04 - val_pearson_correlation: 0.9956\n",
      "Epoch 36/500\n",
      "45/45 [==============================] - 6s 127ms/step - loss: 1.6291e-04 - pearson_correlation: 0.9957 - val_loss: 1.6440e-04 - val_pearson_correlation: 0.9958\n",
      "Epoch 37/500\n",
      "45/45 [==============================] - 6s 126ms/step - loss: 1.6133e-04 - pearson_correlation: 0.9958 - val_loss: 1.6060e-04 - val_pearson_correlation: 0.9960\n",
      "Epoch 38/500\n",
      "45/45 [==============================] - 6s 127ms/step - loss: 1.5571e-04 - pearson_correlation: 0.9959 - val_loss: 1.5899e-04 - val_pearson_correlation: 0.9960\n",
      "Epoch 39/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.5694e-04 - pearson_correlation: 0.9959 - val_loss: 1.6035e-04 - val_pearson_correlation: 0.9958\n",
      "Epoch 40/500\n",
      "45/45 [==============================] - 6s 135ms/step - loss: 1.6311e-04 - pearson_correlation: 0.9958 - val_loss: 1.6498e-04 - val_pearson_correlation: 0.9960\n",
      "Epoch 41/500\n",
      "45/45 [==============================] - 6s 135ms/step - loss: 1.5489e-04 - pearson_correlation: 0.9959 - val_loss: 1.5824e-04 - val_pearson_correlation: 0.9960\n",
      "Epoch 42/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.4950e-04 - pearson_correlation: 0.9961 - val_loss: 1.5800e-04 - val_pearson_correlation: 0.9960\n",
      "Epoch 43/500\n",
      "45/45 [==============================] - 6s 127ms/step - loss: 1.4931e-04 - pearson_correlation: 0.9961 - val_loss: 1.5146e-04 - val_pearson_correlation: 0.9962\n",
      "Epoch 44/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.4739e-04 - pearson_correlation: 0.9962 - val_loss: 1.8467e-04 - val_pearson_correlation: 0.9955\n",
      "Epoch 45/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.5507e-04 - pearson_correlation: 0.9960 - val_loss: 1.5917e-04 - val_pearson_correlation: 0.9960\n",
      "Epoch 46/500\n",
      "45/45 [==============================] - 6s 127ms/step - loss: 1.4844e-04 - pearson_correlation: 0.9961 - val_loss: 1.5557e-04 - val_pearson_correlation: 0.9961\n",
      "Epoch 47/500\n",
      "45/45 [==============================] - 6s 127ms/step - loss: 1.5189e-04 - pearson_correlation: 0.9960 - val_loss: 1.5331e-04 - val_pearson_correlation: 0.9962\n",
      "Epoch 48/500\n",
      "45/45 [==============================] - 6s 127ms/step - loss: 1.5141e-04 - pearson_correlation: 0.9961 - val_loss: 1.4520e-04 - val_pearson_correlation: 0.9963\n",
      "Epoch 49/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.4657e-04 - pearson_correlation: 0.9962 - val_loss: 1.4433e-04 - val_pearson_correlation: 0.9963\n",
      "Epoch 50/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.4585e-04 - pearson_correlation: 0.9962 - val_loss: 1.5331e-04 - val_pearson_correlation: 0.9960\n",
      "Epoch 51/500\n",
      "45/45 [==============================] - 6s 127ms/step - loss: 1.4477e-04 - pearson_correlation: 0.9962 - val_loss: 1.4592e-04 - val_pearson_correlation: 0.9963\n",
      "Epoch 52/500\n",
      "45/45 [==============================] - 6s 127ms/step - loss: 1.4262e-04 - pearson_correlation: 0.9963 - val_loss: 1.4631e-04 - val_pearson_correlation: 0.9963\n",
      "Epoch 53/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.4950e-04 - pearson_correlation: 0.9961 - val_loss: 1.4162e-04 - val_pearson_correlation: 0.9964\n",
      "Epoch 54/500\n",
      "45/45 [==============================] - 6s 130ms/step - loss: 1.4443e-04 - pearson_correlation: 0.9962 - val_loss: 1.5771e-04 - val_pearson_correlation: 0.9960\n",
      "Epoch 55/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.4659e-04 - pearson_correlation: 0.9962 - val_loss: 1.5233e-04 - val_pearson_correlation: 0.9961\n",
      "Epoch 56/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.4951e-04 - pearson_correlation: 0.9961 - val_loss: 1.5470e-04 - val_pearson_correlation: 0.9961\n",
      "Epoch 57/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.4503e-04 - pearson_correlation: 0.9962 - val_loss: 1.5031e-04 - val_pearson_correlation: 0.9961\n",
      "Epoch 58/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.4399e-04 - pearson_correlation: 0.9962 - val_loss: 1.4823e-04 - val_pearson_correlation: 0.9961\n",
      "Epoch 59/500\n",
      "45/45 [==============================] - 6s 127ms/step - loss: 1.4955e-04 - pearson_correlation: 0.9961 - val_loss: 1.6768e-04 - val_pearson_correlation: 0.9957\n",
      "Epoch 60/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.4366e-04 - pearson_correlation: 0.9963 - val_loss: 1.4338e-04 - val_pearson_correlation: 0.9963\n",
      "Epoch 61/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.4186e-04 - pearson_correlation: 0.9963 - val_loss: 1.5043e-04 - val_pearson_correlation: 0.9962\n",
      "Epoch 62/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.5171e-04 - pearson_correlation: 0.9960 - val_loss: 1.4181e-04 - val_pearson_correlation: 0.9965\n",
      "Epoch 63/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.4438e-04 - pearson_correlation: 0.9962 - val_loss: 1.4089e-04 - val_pearson_correlation: 0.9964\n",
      "Epoch 64/500\n",
      "45/45 [==============================] - 6s 133ms/step - loss: 1.4351e-04 - pearson_correlation: 0.9963 - val_loss: 1.4444e-04 - val_pearson_correlation: 0.9963\n",
      "Epoch 65/500\n",
      "45/45 [==============================] - 7s 148ms/step - loss: 1.4252e-04 - pearson_correlation: 0.9963 - val_loss: 1.5251e-04 - val_pearson_correlation: 0.9961\n",
      "Epoch 66/500\n",
      "45/45 [==============================] - 6s 143ms/step - loss: 1.4588e-04 - pearson_correlation: 0.9962 - val_loss: 1.5851e-04 - val_pearson_correlation: 0.9959\n",
      "Epoch 67/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.4879e-04 - pearson_correlation: 0.9961 - val_loss: 1.4268e-04 - val_pearson_correlation: 0.9963\n",
      "Epoch 68/500\n",
      "45/45 [==============================] - 6s 130ms/step - loss: 1.4937e-04 - pearson_correlation: 0.9961 - val_loss: 1.4083e-04 - val_pearson_correlation: 0.9964\n",
      "Epoch 69/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.4306e-04 - pearson_correlation: 0.9963 - val_loss: 1.4118e-04 - val_pearson_correlation: 0.9964\n",
      "Epoch 70/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.4175e-04 - pearson_correlation: 0.9963 - val_loss: 1.6126e-04 - val_pearson_correlation: 0.9959\n",
      "Epoch 71/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.4917e-04 - pearson_correlation: 0.9961 - val_loss: 1.5107e-04 - val_pearson_correlation: 0.9961\n",
      "Epoch 72/500\n",
      "45/45 [==============================] - 6s 131ms/step - loss: 1.3640e-04 - pearson_correlation: 0.9964 - val_loss: 1.4032e-04 - val_pearson_correlation: 0.9964\n",
      "Epoch 73/500\n",
      "45/45 [==============================] - 6s 130ms/step - loss: 1.4135e-04 - pearson_correlation: 0.9963 - val_loss: 1.4060e-04 - val_pearson_correlation: 0.9964\n",
      "Epoch 74/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.3649e-04 - pearson_correlation: 0.9964 - val_loss: 1.4503e-04 - val_pearson_correlation: 0.9963\n",
      "Epoch 75/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.3848e-04 - pearson_correlation: 0.9964 - val_loss: 1.3774e-04 - val_pearson_correlation: 0.9964\n",
      "Epoch 76/500\n",
      "45/45 [==============================] - 6s 132ms/step - loss: 1.3964e-04 - pearson_correlation: 0.9963 - val_loss: 1.4012e-04 - val_pearson_correlation: 0.9964\n",
      "Epoch 77/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.4213e-04 - pearson_correlation: 0.9963 - val_loss: 1.5988e-04 - val_pearson_correlation: 0.9960\n",
      "Epoch 78/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.4209e-04 - pearson_correlation: 0.9963 - val_loss: 1.3741e-04 - val_pearson_correlation: 0.9965\n",
      "Epoch 79/500\n",
      "45/45 [==============================] - 6s 130ms/step - loss: 1.4150e-04 - pearson_correlation: 0.9963 - val_loss: 1.5129e-04 - val_pearson_correlation: 0.9962\n",
      "Epoch 80/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.3784e-04 - pearson_correlation: 0.9964 - val_loss: 1.3939e-04 - val_pearson_correlation: 0.9964\n",
      "Epoch 81/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.3853e-04 - pearson_correlation: 0.9964 - val_loss: 1.3898e-04 - val_pearson_correlation: 0.9965\n",
      "Epoch 82/500\n",
      "45/45 [==============================] - 6s 139ms/step - loss: 1.3585e-04 - pearson_correlation: 0.9964 - val_loss: 1.4675e-04 - val_pearson_correlation: 0.9963\n",
      "Epoch 83/500\n",
      "45/45 [==============================] - 6s 135ms/step - loss: 1.3901e-04 - pearson_correlation: 0.9964 - val_loss: 1.4077e-04 - val_pearson_correlation: 0.9964\n",
      "Epoch 84/500\n",
      "45/45 [==============================] - 6s 138ms/step - loss: 1.3771e-04 - pearson_correlation: 0.9964 - val_loss: 1.4711e-04 - val_pearson_correlation: 0.9962\n",
      "Epoch 85/500\n",
      "45/45 [==============================] - 6s 136ms/step - loss: 1.3475e-04 - pearson_correlation: 0.9965 - val_loss: 1.4683e-04 - val_pearson_correlation: 0.9963\n",
      "Epoch 86/500\n",
      "45/45 [==============================] - 6s 139ms/step - loss: 1.3658e-04 - pearson_correlation: 0.9964 - val_loss: 1.3898e-04 - val_pearson_correlation: 0.9965\n",
      "Epoch 87/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.3541e-04 - pearson_correlation: 0.9965 - val_loss: 1.5326e-04 - val_pearson_correlation: 0.9961\n",
      "Epoch 88/500\n",
      "45/45 [==============================] - 6s 130ms/step - loss: 1.4363e-04 - pearson_correlation: 0.9963 - val_loss: 1.3968e-04 - val_pearson_correlation: 0.9965\n",
      "Epoch 89/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.4118e-04 - pearson_correlation: 0.9963 - val_loss: 1.5443e-04 - val_pearson_correlation: 0.9961\n",
      "Epoch 90/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.3827e-04 - pearson_correlation: 0.9964 - val_loss: 1.4077e-04 - val_pearson_correlation: 0.9964\n",
      "Epoch 91/500\n",
      "45/45 [==============================] - 6s 127ms/step - loss: 1.3804e-04 - pearson_correlation: 0.9964 - val_loss: 1.3488e-04 - val_pearson_correlation: 0.9966\n",
      "Epoch 92/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.3450e-04 - pearson_correlation: 0.9965 - val_loss: 1.3540e-04 - val_pearson_correlation: 0.9966\n",
      "Epoch 93/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.4806e-04 - pearson_correlation: 0.9961 - val_loss: 1.6003e-04 - val_pearson_correlation: 0.9958\n",
      "Epoch 94/500\n",
      "45/45 [==============================] - 6s 129ms/step - loss: 1.4471e-04 - pearson_correlation: 0.9962 - val_loss: 1.4629e-04 - val_pearson_correlation: 0.9962\n",
      "Epoch 95/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 6s 131ms/step - loss: 1.3834e-04 - pearson_correlation: 0.9964 - val_loss: 1.3177e-04 - val_pearson_correlation: 0.9966\n",
      "Epoch 96/500\n",
      "45/45 [==============================] - 6s 132ms/step - loss: 1.3420e-04 - pearson_correlation: 0.9965 - val_loss: 1.4965e-04 - val_pearson_correlation: 0.9963\n",
      "Epoch 97/500\n",
      "45/45 [==============================] - 6s 133ms/step - loss: 1.3939e-04 - pearson_correlation: 0.9964 - val_loss: 1.4042e-04 - val_pearson_correlation: 0.9964\n",
      "Epoch 98/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.3628e-04 - pearson_correlation: 0.9964 - val_loss: 1.4514e-04 - val_pearson_correlation: 0.9964\n",
      "Epoch 99/500\n",
      "45/45 [==============================] - 6s 131ms/step - loss: 1.3471e-04 - pearson_correlation: 0.9965 - val_loss: 1.4638e-04 - val_pearson_correlation: 0.9962\n",
      "Epoch 100/500\n",
      "45/45 [==============================] - 6s 128ms/step - loss: 1.3247e-04 - pearson_correlation: 0.9965 - val_loss: 1.3977e-04 - val_pearson_correlation: 0.9964\n",
      "Epoch 101/500\n",
      "45/45 [==============================] - 6s 132ms/step - loss: 1.3634e-04 - pearson_correlation: 0.9964 - val_loss: 1.3648e-04 - val_pearson_correlation: 0.9965\n",
      "Epoch 102/500\n",
      "45/45 [==============================] - 6s 132ms/step - loss: 1.3934e-04 - pearson_correlation: 0.9964 - val_loss: 1.3198e-04 - val_pearson_correlation: 0.9966\n",
      "Epoch 103/500\n",
      "45/45 [==============================] - 6s 131ms/step - loss: 1.3119e-04 - pearson_correlation: 0.9966 - val_loss: 1.3951e-04 - val_pearson_correlation: 0.9964\n",
      "Epoch 104/500\n",
      "45/45 [==============================] - 6s 130ms/step - loss: 1.3225e-04 - pearson_correlation: 0.9965 - val_loss: 1.3762e-04 - val_pearson_correlation: 0.9965\n",
      "Epoch 105/500\n",
      "45/45 [==============================] - 6s 130ms/step - loss: 1.3640e-04 - pearson_correlation: 0.9965 - val_loss: 1.3669e-04 - val_pearson_correlation: 0.9965\n",
      "Epoch 106/500\n",
      "45/45 [==============================] - 6s 130ms/step - loss: 1.3649e-04 - pearson_correlation: 0.9964 - val_loss: 1.5423e-04 - val_pearson_correlation: 0.9960\n",
      "Epoch 107/500\n",
      "45/45 [==============================] - 6s 130ms/step - loss: 1.3749e-04 - pearson_correlation: 0.9964 - val_loss: 1.4202e-04 - val_pearson_correlation: 0.9964\n",
      "Epoch 108/500\n"
     ]
    }
   ],
   "source": [
    "#iterate through the folds\n",
    "for i,(trainingIndices,testingIndices) in enumerate(splitter.split(allElementalAbundances[:,0])):\n",
    "    #print the current fold number\n",
    "    print(f\"Starting processing fold {i+1}\")\n",
    "    \n",
    "    #build the model\n",
    "    jointAbundanceEstimator=tf.keras.Model(inputs=inputLayer,\n",
    "                                       outputs=[abundanceValues],\n",
    "                                       name=\"Multitask_Abundance_estimator\")\n",
    "    \n",
    "    #compile the model\n",
    "    jointAbundanceEstimator.compile(optimizer=optimizer,\n",
    "                                loss=lossFunction,\n",
    "                                metrics=[pearson_correlation],\n",
    "                                steps_per_execution=45)\n",
    "    \n",
    "    #fit the model to the current fold's data\n",
    "    currentModelTrainingHistory=jointAbundanceEstimator.fit(x=preprocessedSpectra[trainingIndices,:],\n",
    "                                                            y=allElementalAbundances[trainingIndices,:],\n",
    "                                                            batch_size=batchSize,\n",
    "                                                            epochs=noOfEpochs,\n",
    "                                                            validation_data=(preprocessedSpectra[testingIndices,:],\n",
    "                                                                             allElementalAbundances[testingIndices,:]),\n",
    "                                                            validation_freq=1)\n",
    "                                                            \n",
    "    \n",
    "    #save the training history of the current model\n",
    "    modelTrainingHistories.append(currentModelTrainingHistory)\n",
    "    \n",
    "    \n",
    "    #set the address where the trained model from the current \n",
    "    currentFoldTrainedModelAddress=\"C:/ML4Sci/Ml4Sci_GRS_abundance_estimation/Models/untrained_Joint_Abundance_Attention_Model\"+str(i+1)+\".h5\"\n",
    "    \n",
    "    #save the trained model\n",
    "    trainedModels.append(kAbundanceEstimator)\n",
    "    \n",
    "    #remove the trained model from memory\n",
    "    kAbundanceEstimator=None\n",
    "    del kAbundanceEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a0a2bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
