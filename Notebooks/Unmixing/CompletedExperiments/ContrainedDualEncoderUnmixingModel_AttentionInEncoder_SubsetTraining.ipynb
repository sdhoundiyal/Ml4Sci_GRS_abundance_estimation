{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83832900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#expand cell width to 100%\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276ccc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas \n",
    "import pandas as pd\n",
    "#import numpy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the address of the dataframe containing the pre-processed dataset\n",
    "dataFramePickleAddress=\"C:/ML4Sci/Ml4Sci_GRS_abundance_estimation/Dataset/GRSFiveDegreeSectionPreProcessedDataset.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b151d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataframe\n",
    "dataframe=pd.read_pickle(dataFramePickleAddress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac4239",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the dataframe\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79629eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a variable epsilon, which contains a very small value that is added to all inputs and outputs to make sure no values are 0\n",
    "epsilon=1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ab13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the abundance values for the seven elements for which the model is to be trained\n",
    "#aluminium\n",
    "aluminiumAbundances=dataframe['Aluminum'].to_numpy()+epsilon\n",
    "#calcium\n",
    "calciumAbundances=dataframe['Calcium'].to_numpy()+epsilon\n",
    "#iron\n",
    "ironAbundances=dataframe['Iron'].to_numpy()+epsilon\n",
    "#magnesium\n",
    "magnesiumAbundances=dataframe['Magnesium'].to_numpy()+epsilon\n",
    "#oxygen\n",
    "oxygenAbundances=dataframe['Oxygen'].to_numpy()+epsilon\n",
    "#silicon\n",
    "siliconAbundances=dataframe['Silicon'].to_numpy()+epsilon\n",
    "#titanium\n",
    "titaniumAbundances=dataframe['Titanium'].to_numpy()+epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3460a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get abundance values for the three nuclear elements\n",
    "potassiumAbundances=dataframe['Potassium'].to_numpy()+epsilon\n",
    "thoriumAbundances=dataframe['Thorium'].to_numpy()+epsilon\n",
    "uraniumAbundances=dataframe['Uranium'].to_numpy()+epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8638aab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale nuclear abundance values between 0 and 1\n",
    "potassiumAbundances=(potassiumAbundances-np.amin(potassiumAbundances))/np.ptp(potassiumAbundances)\n",
    "thoriumAbundances=(thoriumAbundances-np.amin(thoriumAbundances))/np.ptp(thoriumAbundances)\n",
    "uraniumAbundances=(uraniumAbundances-np.amin(uraniumAbundances))/np.ptp(uraniumAbundances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78ae584",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an list which contains the element names to be estimated\n",
    "regularElementNames=['Aluminum',\n",
    "                     'Calcium',\n",
    "                     'Iron',\n",
    "                     'Magnesium',\n",
    "                     'Oxygen',\n",
    "                     'Silicon',\n",
    "                     'Titanium']\n",
    "#conver the list to a numpy array\n",
    "regularElementNames=np.array(regularElementNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f5325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the elemental abundances into a single matrix\n",
    "regularElementalAbundances=np.dstack([aluminiumAbundances,\n",
    "                                      calciumAbundances,\n",
    "                                      ironAbundances,\n",
    "                                      magnesiumAbundances,\n",
    "                                      oxygenAbundances,\n",
    "                                      siliconAbundances,\n",
    "                                      titaniumAbundances])\n",
    "#reshape the abundance matrix\n",
    "regularElementalAbundances=regularElementalAbundances[0,:,:]\n",
    "#rescale weigth percent values from % (0-100) to franctions (0-1)\n",
    "regularElementalAbundances=regularElementalAbundances/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7dd49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an list which contains the element names to be estimated\n",
    "nuclearElementNames=['Potassium',\n",
    "                     'Thorium',\n",
    "                     'Uranium']\n",
    "#conver the list to a numpy array\n",
    "nuclearElementNames=np.array(nuclearElementNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33eee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array containing the names of all elements\n",
    "allElementNames=np.hstack([regularElementNames,nuclearElementNames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4b854",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine the elemental abundances into a single matrix\n",
    "nuclearElementalAbundances=np.dstack([potassiumAbundances,\n",
    "                                      thoriumAbundances,\n",
    "                                      uraniumAbundances])\n",
    "#reshape the abundance matrix\n",
    "nuclearElementalAbundances=nuclearElementalAbundances[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e32348",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the pre-processed spectra as a numpy array\n",
    "preprocessedSpectra=dataframe['Normalized Continuum Removed Denoised Log Scaled Spectra'].to_numpy()\n",
    "#reshape the numpy array\n",
    "preprocessedSpectra=np.vstack(preprocessedSpectra)+epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6d40e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the eneergy values for each band\n",
    "gain=17.8 #keV/channel\n",
    "energyBands=np.arange(0,512,1)*gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa371af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the index of the last relavent band\n",
    "finalRelaventBandIndex=np.argmin(np.abs(energyBands-8000))\n",
    "#compute the index of the first relavent band\n",
    "firstRelaventBandIndex=finalRelaventBandIndex-preprocessedSpectra.shape[1]+1\n",
    "#get the energies of the relavent bands\n",
    "relaventEnergyBands=energyBands[firstRelaventBandIndex:finalRelaventBandIndex+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5cf87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the variables no longer needed from memory\n",
    "del firstRelaventBandIndex\n",
    "del finalRelaventBandIndex\n",
    "del energyBands\n",
    "del gain\n",
    "del dataframe\n",
    "del dataFramePickleAddress\n",
    "del aluminiumAbundances\n",
    "del calciumAbundances\n",
    "del ironAbundances\n",
    "del magnesiumAbundances\n",
    "del oxygenAbundances\n",
    "del siliconAbundances\n",
    "del titaniumAbundances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyplot from matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0badb81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set plot parameters\n",
    "baseFontSize=18\n",
    "noOfBinsForHistogram=100\n",
    "noOfXticks=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31fb567",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure \n",
    "figure,axes=plt.subplots(nrows=1,\n",
    "                         ncols=7,\n",
    "                         figsize=(35,5),\n",
    "                         dpi=200)\n",
    "#iterate over all elements\n",
    "for i in range(regularElementNames.shape[0]):\n",
    "    #plot the histogram\n",
    "    temp=axes[i].hist(regularElementalAbundances[:,i],\n",
    "                      bins=noOfBinsForHistogram)\n",
    "    #set the title of the figure\n",
    "    axes[i].set_title(regularElementNames[i],fontsize=baseFontSize*1.2)\n",
    "    #set the axis labels\n",
    "    axes[i].set_xlabel(\"Wt frac.\",fontsize=baseFontSize*1.2)\n",
    "    axes[i].set_ylabel(\"Freq\",fontsize=baseFontSize*1.2)\n",
    "    #set the ticks and their label sizes\n",
    "    axes[i].set_xticks(np.arange(np.amin(regularElementalAbundances[:,i]),\n",
    "                                 np.amax(regularElementalAbundances[:,i])+np.ptp(regularElementalAbundances[:,i])/noOfXticks,\n",
    "                                 np.ptp(regularElementalAbundances[:,i])/noOfXticks),\n",
    "                       labels=np.round(np.arange(np.amin(regularElementalAbundances[:,i]),\n",
    "                                                 np.amax(regularElementalAbundances[:,i])+np.ptp(regularElementalAbundances[:,i])/noOfXticks,\n",
    "                                                 np.ptp(regularElementalAbundances[:,i])/noOfXticks),\n",
    "                                       2),\n",
    "                       fontsize=baseFontSize)\n",
    "    #set the margins\n",
    "    axes[i].margins(0.01)\n",
    "    \n",
    "#add a title\n",
    "figure.suptitle(\"Distribution of abundance values\",\n",
    "                fontsize=baseFontSize*1.5)\n",
    "#adjust the layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure \n",
    "figure,axes=plt.subplots(nrows=1,\n",
    "                         ncols=3,\n",
    "                         figsize=(15,5),\n",
    "                         dpi=200)\n",
    "#iterate over all elements\n",
    "for i in range(nuclearElementNames.shape[0]):\n",
    "    #plot the histogram\n",
    "    temp=axes[i].hist(nuclearElementalAbundances[:,i],\n",
    "                      bins=noOfBinsForHistogram)\n",
    "    #set the title of the figure\n",
    "    axes[i].set_title(nuclearElementNames[i],fontsize=baseFontSize*1.2)\n",
    "    #set the axis labels\n",
    "    axes[i].set_xlabel(\"PPM\",fontsize=baseFontSize*1.2)\n",
    "    axes[i].set_ylabel(\"Freq\",fontsize=baseFontSize*1.2)\n",
    "    #set the ticks and their label sizes\n",
    "    axes[i].set_xticks(np.arange(np.amin(nuclearElementalAbundances[:,i]),\n",
    "                                 np.amax(nuclearElementalAbundances[:,i])+np.ptp(nuclearElementalAbundances[:,i])/noOfXticks,\n",
    "                                 np.ptp(nuclearElementalAbundances[:,i])/noOfXticks),\n",
    "                       labels=np.round(np.arange(np.amin(nuclearElementalAbundances[:,i]),\n",
    "                                                 np.amax(nuclearElementalAbundances[:,i])+np.ptp(nuclearElementalAbundances[:,i])/noOfXticks,\n",
    "                                                 np.ptp(nuclearElementalAbundances[:,i])/noOfXticks),\n",
    "                                       2),\n",
    "                       fontsize=baseFontSize)\n",
    "    #set the margins\n",
    "    axes[i].margins(0.01)\n",
    "    \n",
    "#add a title\n",
    "figure.suptitle(\"Distribution of abundance values\",\n",
    "                fontsize=baseFontSize*1.5)\n",
    "#adjust the layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f21cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure\n",
    "plt.figure(figsize=(30,15),\n",
    "           dpi=100)\n",
    "#and plot all the spectra\n",
    "for i in range(preprocessedSpectra.shape[0]):\n",
    "    plt.plot(relaventEnergyBands,\n",
    "             preprocessedSpectra[i,:],\n",
    "             lw=5)\n",
    "#annotate the figure\n",
    "plt.title(\"Preprocessed GRS Spectra\",\n",
    "          fontsize=baseFontSize*1.8)\n",
    "plt.xticks(np.arange(np.amin(relaventEnergyBands),\n",
    "                     np.amax(relaventEnergyBands)+np.ptp(relaventEnergyBands)/noOfXticks,\n",
    "                     np.ptp(relaventEnergyBands)/noOfXticks),\n",
    "           fontsize=baseFontSize*1.2)\n",
    "plt.yticks(np.arange(np.amin(preprocessedSpectra),\n",
    "                     np.amax(preprocessedSpectra)+np.ptp(preprocessedSpectra)/noOfXticks,\n",
    "                     np.ptp(preprocessedSpectra)/noOfXticks),\n",
    "           labels=np.round(np.arange(np.amin(preprocessedSpectra),\n",
    "                                     np.amax(preprocessedSpectra)+np.ptp(preprocessedSpectra)/noOfXticks,\n",
    "                                     np.ptp(preprocessedSpectra)/noOfXticks),\n",
    "                           2),\n",
    "           fontsize=baseFontSize*1.2)\n",
    "plt.xlabel(\"KeV\",\n",
    "           fontsize=baseFontSize*1.5)\n",
    "plt.ylabel(\"log(Counts/min) ratioed\",\n",
    "           fontsize=baseFontSize*1.5)\n",
    "plt.margins(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f788bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import PCA from sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "#import cosine distance from scipy\n",
    "from scipy.spatial.distance import cosine as cosineDistance\n",
    "\n",
    "#define a function which given an array of spectra, explained-variance ratio, and no of spectra to be selected\n",
    "#returns the indices of most interesting (most unlike the others) spectra\n",
    "def extractMostExtremeSpectraWithDEMUD(spectra,varianceToExplain,noOfSpectraToRetrive):\n",
    "    #create an array to hold the indices of the selected (interesting) super-pixels\n",
    "    selectedSpectraIndices=[]\n",
    "    #create a PCA object which explains over 95% of the variance in the data\n",
    "    pcaObject=PCA(n_components=varianceToExplain,\n",
    "                  svd_solver='auto')\n",
    "    #compute the PCA model for the spectra and use it to reconstruct the orginal spectra\n",
    "    reconstructedSpectra=pcaObject.inverse_transform(pcaObject.fit_transform(spectra))\n",
    "    #create an array to save the reconstruction error (cosine distance)\n",
    "    reconstructionErrors=np.full(spectra.shape[0],\n",
    "                                 np.nan)\n",
    "    #compute the cosine distance between the orginal and reconstructed spectra\n",
    "    for i in range(spectra.shape[0]):\n",
    "        reconstructionErrors[i]=cosineDistance(spectra[i,:],\n",
    "                                               reconstructedSpectra[i,:])\n",
    "    #save the index of the spectra with the greatest error\n",
    "    selectedSpectraIndices.append(np.argmax(reconstructionErrors))\n",
    "\n",
    "    #iteratively extract the most dissimar spectra\n",
    "    for i in range(noOfSpectraToRetrive-1):\n",
    "        #fit PCA to the selected spectra\n",
    "        pcaObject.fit(spectra[selectedSpectraIndices,:])\n",
    "        #apply PCA and then reconstruct the spectra\n",
    "        reconstructedSpectra=pcaObject.inverse_transform(pcaObject.transform(spectra))\n",
    "        #create an array to save the reconstruction error (cosine distance)\n",
    "        reconstructionErrors=np.full(spectra.shape[0],np.nan)\n",
    "        #compute the cosine distance between the orginal and reconstructed spectra\n",
    "        for i in range(spectra.shape[0]):\n",
    "            reconstructionErrors[i]=cosineDistance(spectra[i,:],\n",
    "                                                   reconstructedSpectra[i,:])\n",
    "        #remove the error of the selected pixels\n",
    "        reconstructionErrors=np.delete(reconstructionErrors,\n",
    "                                       selectedSpectraIndices)\n",
    "        #get the index of the spectra with the greatest error\n",
    "        selectedSpectraIndices.append(np.delete(np.arange(0,spectra.shape[0],1),\n",
    "                                                selectedSpectraIndices,axis=0)[np.argmax(reconstructionErrors)])\n",
    "        \n",
    "    return selectedSpectraIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61792c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set parameters for extracting differing spectra\n",
    "varianceToBeExplainedDuringDEMUD=0.98\n",
    "noOfExtremeSpectraToBeRetrieved=179\n",
    "#get indices of the most extreme spectra\n",
    "extremeIndices=extractMostExtremeSpectraWithDEMUD(preprocessedSpectra,\n",
    "                                                  varianceToExplain=varianceToBeExplainedDuringDEMUD,\n",
    "                                                  noOfSpectraToRetrive=noOfExtremeSpectraToBeRetrieved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50b6ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the subset data\n",
    "#spectra\n",
    "preprocessedSpectra=preprocessedSpectra[extremeIndices,:]\n",
    "#regular abundances\n",
    "regularElementalAbundances=regularElementalAbundances[extremeIndices,:]\n",
    "#nuclear abundances\n",
    "nuclearElementalAbundances=nuclearElementalAbundances[extremeIndices,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ce3ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure \n",
    "figure,axes=plt.subplots(nrows=1,\n",
    "                         ncols=7,\n",
    "                         figsize=(35,5),\n",
    "                         dpi=200)\n",
    "#iterate over all elements\n",
    "for i in range(regularElementNames.shape[0]):\n",
    "    #plot the histogram\n",
    "    temp=axes[i].hist(regularElementalAbundances[:,i],\n",
    "                      bins=noOfBinsForHistogram)\n",
    "    #set the title of the figure\n",
    "    axes[i].set_title(regularElementNames[i],fontsize=baseFontSize*1.2)\n",
    "    #set the axis labels\n",
    "    axes[i].set_xlabel(\"Wt frac.\",fontsize=baseFontSize*1.2)\n",
    "    axes[i].set_ylabel(\"Freq\",fontsize=baseFontSize*1.2)\n",
    "    #set the ticks and their label sizes\n",
    "    axes[i].set_xticks(np.arange(np.amin(regularElementalAbundances[:,i]),\n",
    "                                 np.amax(regularElementalAbundances[:,i])+np.ptp(regularElementalAbundances[:,i])/noOfXticks,\n",
    "                                 np.ptp(regularElementalAbundances[:,i])/noOfXticks),\n",
    "                       labels=np.round(np.arange(np.amin(regularElementalAbundances[:,i]),\n",
    "                                                 np.amax(regularElementalAbundances[:,i])+np.ptp(regularElementalAbundances[:,i])/noOfXticks,\n",
    "                                                 np.ptp(regularElementalAbundances[:,i])/noOfXticks),\n",
    "                                       2),\n",
    "                       fontsize=baseFontSize)\n",
    "    #set the margins\n",
    "    axes[i].margins(0.01)\n",
    "    \n",
    "#add a title\n",
    "figure.suptitle(\"Distribution of abundance values\",\n",
    "                fontsize=baseFontSize*1.5)\n",
    "#adjust the layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f49b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure \n",
    "figure,axes=plt.subplots(nrows=1,\n",
    "                         ncols=3,\n",
    "                         figsize=(15,5),\n",
    "                         dpi=200)\n",
    "#iterate over all elements\n",
    "for i in range(nuclearElementNames.shape[0]):\n",
    "    #plot the histogram\n",
    "    temp=axes[i].hist(nuclearElementalAbundances[:,i],\n",
    "                      bins=noOfBinsForHistogram)\n",
    "    #set the title of the figure\n",
    "    axes[i].set_title(nuclearElementNames[i],fontsize=baseFontSize*1.2)\n",
    "    #set the axis labels\n",
    "    axes[i].set_xlabel(\"PPM\",fontsize=baseFontSize*1.2)\n",
    "    axes[i].set_ylabel(\"Freq\",fontsize=baseFontSize*1.2)\n",
    "    #set the ticks and their label sizes\n",
    "    axes[i].set_xticks(np.arange(np.amin(nuclearElementalAbundances[:,i]),\n",
    "                                 np.amax(nuclearElementalAbundances[:,i])+np.ptp(nuclearElementalAbundances[:,i])/noOfXticks,\n",
    "                                 np.ptp(nuclearElementalAbundances[:,i])/noOfXticks),\n",
    "                       labels=np.round(np.arange(np.amin(nuclearElementalAbundances[:,i]),\n",
    "                                                 np.amax(nuclearElementalAbundances[:,i])+np.ptp(nuclearElementalAbundances[:,i])/noOfXticks,\n",
    "                                                 np.ptp(nuclearElementalAbundances[:,i])/noOfXticks),\n",
    "                                       2),\n",
    "                       fontsize=baseFontSize)\n",
    "    #set the margins\n",
    "    axes[i].margins(0.01)\n",
    "    \n",
    "#add a title\n",
    "figure.suptitle(\"Distribution of abundance values\",\n",
    "                fontsize=baseFontSize*1.5)\n",
    "#adjust the layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8937b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure\n",
    "plt.figure(figsize=(30,15),\n",
    "           dpi=100)\n",
    "#and plot all the spectra\n",
    "for i in range(preprocessedSpectra.shape[0]):\n",
    "    plt.plot(relaventEnergyBands,\n",
    "             preprocessedSpectra[i,:],\n",
    "             lw=5)\n",
    "#annotate the figure\n",
    "plt.title(\"Preprocessed GRS Spectra\",\n",
    "          fontsize=baseFontSize*1.8)\n",
    "plt.xticks(np.arange(np.amin(relaventEnergyBands),\n",
    "                     np.amax(relaventEnergyBands)+np.ptp(relaventEnergyBands)/noOfXticks,\n",
    "                     np.ptp(relaventEnergyBands)/noOfXticks),\n",
    "           fontsize=baseFontSize*1.2)\n",
    "plt.yticks(np.arange(np.amin(preprocessedSpectra),\n",
    "                     np.amax(preprocessedSpectra)+np.ptp(preprocessedSpectra)/noOfXticks,\n",
    "                     np.ptp(preprocessedSpectra)/noOfXticks),\n",
    "           labels=np.round(np.arange(np.amin(preprocessedSpectra),\n",
    "                                     np.amax(preprocessedSpectra)+np.ptp(preprocessedSpectra)/noOfXticks,\n",
    "                                     np.ptp(preprocessedSpectra)/noOfXticks),\n",
    "                           2),\n",
    "           fontsize=baseFontSize*1.2)\n",
    "plt.xlabel(\"KeV\",\n",
    "           fontsize=baseFontSize*1.5)\n",
    "plt.ylabel(\"log(Counts/min) ratioed\",\n",
    "           fontsize=baseFontSize*1.5)\n",
    "plt.margins(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e062271",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082ed242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set a global seed value\n",
    "globalSeed=23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b161628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the number of channels in the pre-processed spectra\n",
    "noOfChannels=preprocessedSpectra.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b06b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an input layer\n",
    "inputLayer=tf.keras.Input(shape=(noOfChannels,\n",
    "                                 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c03c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a funtion which creates the CAM (Channel Attention Module)\n",
    "def createCAM(inputFeatureBlock,reductionRatio):\n",
    "    #perform max pooling along the channel dimension\n",
    "    channelMaxPooledFeatures=tf.math.reduce_max(inputFeatureBlock,\n",
    "                                                axis=1,\n",
    "                                                keepdims=False)\n",
    "    \n",
    "    #perform avg pooling along the channel dimension\n",
    "    channelAvgPooledFeatures=tf.math.reduce_mean(inputFeatureBlock,\n",
    "                                                 axis=1,\n",
    "                                                 keepdims=False)\n",
    "    \n",
    "    #create the bottleneck for the MLP\n",
    "    bottleneckLayer=tf.keras.layers.Dense(channelAvgPooledFeatures.shape[-1]//reductionRatio,\n",
    "                                          activation='relu')\n",
    "    \n",
    "    #create the recontruction layer for the MLP\n",
    "    outputLayer=tf.keras.layers.Dense(channelAvgPooledFeatures.shape[-1],\n",
    "                                      activation='relu')\n",
    "    \n",
    "    \n",
    "    #pass the max pooled features through the bottle-neck\n",
    "    reconstructeedMaxPooledFeatures=outputLayer(bottleneckLayer(channelMaxPooledFeatures))\n",
    "    \n",
    "    \n",
    "    #pass the avg pooled features through the bottle-neck\n",
    "    reconstructeedAvgPooledFeatures=outputLayer(bottleneckLayer(channelMaxPooledFeatures))\n",
    "    \n",
    "    \n",
    "    #add the two reconstructed features together\n",
    "    summedFeatures=tf.math.add(reconstructeedMaxPooledFeatures,\n",
    "                               reconstructeedAvgPooledFeatures)\n",
    "    \n",
    "    #apply sigmoid activation to the summed features to get the channel attention map\n",
    "    channelAttentionMap=tf.keras.activations.sigmoid(summedFeatures)\n",
    "    \n",
    "    #return the channel attention map\n",
    "    return channelAttentionMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa32a277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to create the Spatial attention module (SAM)\n",
    "def createSAM(inputFeatureBlock,kernelSize):\n",
    "    #perform max pooling on the input features\n",
    "    maxPooledFeatureMap=tf.math.reduce_max(inputFeatureBlock,\n",
    "                                           axis=-1,\n",
    "                                           keepdims=False)\n",
    "    #perform average pooling on the input features\n",
    "    averagePooledFeatureMap=tf.math.reduce_mean(inputFeatureBlock,\n",
    "                                                 axis=-1,\n",
    "                                                 keepdims=False)\n",
    "    \n",
    "    #concatenate the feature maps together\n",
    "    concatenatedFeatureMaps=tf.concat([tf.expand_dims(maxPooledFeatureMap,\n",
    "                                                      axis=-1),\n",
    "                                       tf.expand_dims(averagePooledFeatureMap,\n",
    "                                                      axis=-1)],\n",
    "                                      axis=-1)\n",
    "    \n",
    "    #create the convolutional layer to be applied to the concatenated feature map\n",
    "    convolutionLayer=tf.keras.layers.Conv1D(filters=1,\n",
    "                                            kernel_size=kernelSize,\n",
    "                                            strides=1,\n",
    "                                            padding='same',\n",
    "                                            activation='sigmoid')\n",
    "    #create and add a dropout layer\n",
    "    spatialDropoutLayer=tf.keras.layers.Dropout(0.5,\n",
    "                                                noise_shape=None,\n",
    "                                                seed=globalSeed)\n",
    "    \n",
    "    \n",
    "    #get the spatial attention map\n",
    "    spatialAttentionMap=convolutionLayer(concatenatedFeatureMaps)\n",
    "    \n",
    "    \n",
    "    #return the channel attention map\n",
    "    return spatialAttentionMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5350d196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function which creates a CBAM block\n",
    "def createCBAM(inputLayer,convolutionalKernelSize,noOfConvolutionalFilters,reductionRatio,spatialKernelSize):\n",
    "    #create a convolutional layer\n",
    "    convolutionalLayer=tf.keras.layers.Conv1D(filters=noOfConvolutionalFilters,\n",
    "                                              kernel_size=convolutionalKernelSize,\n",
    "                                              strides=1,\n",
    "                                              padding='same')\n",
    "    #get the feature block from the convolutional layer\n",
    "    convolutionalFeatures=convolutionalLayer(inputLayer)\n",
    "    \n",
    "    #get the channel attention map\n",
    "    channelAttentionMap=createCAM(convolutionalFeatures,\n",
    "                                  reductionRatio)\n",
    "    \n",
    "    #replicate the channel attention to make it multiplicative with the features\n",
    "    replicatedChannelAttentionMaps=tf.expand_dims(channelAttentionMap,\n",
    "                                                  axis=1)\n",
    "    replicatedChannelAttentionMaps=tf.repeat(replicatedChannelAttentionMaps,\n",
    "                                             convolutionalFeatures.shape[1],\n",
    "                                             axis=1)\n",
    "    \n",
    "    #compute the channel refined feature by performing element-wise multiplication between the features and the channel attention maps\n",
    "    channelRefinedFeatures=tf.math.multiply(replicatedChannelAttentionMaps,\n",
    "                                            convolutionalFeatures)\n",
    "    \n",
    "    #get the spatial attention map\n",
    "    spatialAttentionMap=createSAM(channelRefinedFeatures,\n",
    "                                  spatialKernelSize)\n",
    "    \n",
    "    #replicate the attention map to make it multiplicative with the channel-refined features\n",
    "    replicatedSpatialAttentionMaps=tf.repeat(spatialAttentionMap,\n",
    "                                             channelRefinedFeatures.shape[-1],\n",
    "                                             axis=-1)\n",
    "    \n",
    "    #multiply the attention map with the channel refined features\n",
    "    spatiallyRefinedFeatures=tf.math.multiply(replicatedSpatialAttentionMaps,\n",
    "                                              channelRefinedFeatures)\n",
    "    \n",
    "    \n",
    "    #add the refined features to the original features\n",
    "    refinedFeatures=tf.math.add(convolutionalFeatures,\n",
    "                                spatiallyRefinedFeatures)\n",
    "    \n",
    "    #return the refined features (i.e. the output of the CBAM)\n",
    "    return refinedFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e5a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function which given a feature volume, applies a single convolutional block to it\n",
    "#the convolution block consists of\n",
    "#Convolutional layer; Activation; Batch Normalization; Dropout\n",
    "#the number of filters, their sizes, stride,, and dropout rate are specified\n",
    "def createConvolutionalBlock(inputVolume,noOfFilters,kernelSize,strideSize,dropoutRate,noOfCBAMBLocks):\n",
    "    #create a convolutional block\n",
    "    convolutionalLayer=tf.keras.layers.Conv1D(filters=noOfFilters,\n",
    "                                                   kernel_size=kernelSize,\n",
    "                                                   strides=strideSize,\n",
    "                                                   padding='valid')\n",
    "    #add the 1st Conv layer to the graph\n",
    "    volume=convolutionalLayer(inputVolume)\n",
    "\n",
    "    #apply Relu activation\n",
    "    preluActivation=tf.keras.layers.PReLU()\n",
    "    #add the 1st activation layer to the graph\n",
    "    volume=preluActivation(volume)\n",
    "\n",
    "    #apply batch normalization\n",
    "    batchNormalization=tf.keras.layers.BatchNormalization()\n",
    "    #add the 1st batch-norm layer to the graph\n",
    "    volume=batchNormalization(volume)\n",
    "\n",
    "    #apply dropout\n",
    "    dropoutLayer=tf.keras.layers.Dropout(dropoutRate,\n",
    "                                         noise_shape=None,\n",
    "                                         seed=globalSeed)\n",
    "    #add the 1st dropout layer to the graph\n",
    "    volume=dropoutLayer(volume)\n",
    "    \n",
    "    #add the specified number of CBAM blocks\n",
    "    for i in range(noOfCBAMBLocks):\n",
    "        #add the CBAM module\n",
    "        volume=createCBAM(volume,\n",
    "                          convolutionalKernelSize=kernelSize,\n",
    "                          noOfConvolutionalFilters=noOfFilters,\n",
    "                          reductionRatio=2,\n",
    "                          spatialKernelSize=3)\n",
    "        \n",
    "        \n",
    "        #apply batch norm over the refined features\n",
    "        batchNormalization=tf.keras.layers.BatchNormalization()\n",
    "        #add the 1st batch-norm layer to the graph\n",
    "        volume=batchNormalization(volume)\n",
    "        \n",
    "        #apply dropout\n",
    "        dropoutLayer=tf.keras.layers.Dropout(dropoutRate,\n",
    "                                             noise_shape=None,\n",
    "                                             seed=globalSeed)\n",
    "        #add the 1st dropout layer to the graph\n",
    "        volume=dropoutLayer(volume)\n",
    "    \n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668c2cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list containing the number of features to be outputted by each Conv block\n",
    "noOfChannelsInEachConvBlock=[128,256,512,1024]\n",
    "#create a list containing the sizes filter for each Conv block\n",
    "filterSizesForEachConvBlock=[7,5,3,3]\n",
    "#create a list containing the strides for each Conv block\n",
    "strideSizesForEachConvBlock=[3,3,2,2]\n",
    "#create a list containing the dropout rate for each Conv block\n",
    "dropoutForEachConvBlock=[0.3,0.3,0.3,0.3]\n",
    "#create a list containing the number of CBAM blocks per convolutional block\n",
    "attentionBlocksPerConvolutionBlock=[4,4,4,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61407cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a NN (Neural Network) graph containing just the input layer\n",
    "regularElementFeatures=inputLayer\n",
    "nuclearElementFeatures=inputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85abf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Convolutional blocks to create the feature extractor for regular elements\n",
    "\n",
    "for i in range(len(noOfChannelsInEachConvBlock)):\n",
    "    #create a convolutional block\n",
    "    regularElementFeatures=createConvolutionalBlock(regularElementFeatures,\n",
    "                                                    noOfChannelsInEachConvBlock[i],\n",
    "                                                    filterSizesForEachConvBlock[i],\n",
    "                                                    strideSizesForEachConvBlock[i],\n",
    "                                                    dropoutForEachConvBlock[i],\n",
    "                                                    attentionBlocksPerConvolutionBlock[i])\n",
    "    \n",
    "    print(f\"Shape of the tensor outputted by the 0th Conv. Block {regularElementFeatures.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fca692",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten the features for the regular elemnents\n",
    "regularElementFeatures=tf.keras.layers.Flatten()(regularElementFeatures)\n",
    "print(f\"Shape of flattened features {regularElementFeatures.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04e85c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list containing the number of features to be outputted by each Conv block\n",
    "noOfChannelsInEachConvBlock=[128,256,512]\n",
    "#create a list containing the sizes filter for each Conv block\n",
    "filterSizesForEachConvBlock=[7,5,3]\n",
    "#create a list containing the strides for each Conv block\n",
    "strideSizesForEachConvBlock=[3,3,2]\n",
    "#create a list containing the dropout rate for each Conv block\n",
    "dropoutForEachConvBlock=[0.3,0.3,0.3,0.3]\n",
    "#create a list containing the number of CBAM blocks per convolutional block\n",
    "attentionBlocksPerConvolutionBlock=[2,2,2,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e007c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add Convolutional blocks to create the feature extractor for regular elements\n",
    "\n",
    "for i in range(len(noOfChannelsInEachConvBlock)):\n",
    "    #create a convolutional block\n",
    "    nuclearElementFeatures=createConvolutionalBlock(nuclearElementFeatures,\n",
    "                                                    noOfChannelsInEachConvBlock[i],\n",
    "                                                    filterSizesForEachConvBlock[i],\n",
    "                                                    strideSizesForEachConvBlock[i],\n",
    "                                                    dropoutForEachConvBlock[i],\n",
    "                                                    attentionBlocksPerConvolutionBlock[i])\n",
    "    \n",
    "    print(f\"Shape of the tensor outputted by the 0th Conv. Block {nuclearElementFeatures.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b972bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten the features for the nuclear elements\n",
    "nuclearElementFeatures=tf.keras.layers.Flatten()(nuclearElementFeatures)\n",
    "print(f\"Shape of flattened features {nuclearElementFeatures.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332658fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the regular elements' abundances from the generated features\n",
    "#it consists of three steps\n",
    "#1. Apply a dense layer with 7 nodes without any activation\n",
    "#2. Compute the absolute value of the values computed by dense layer\n",
    "#3. Compute the l1-norm of the 7 regular elements and and divide them by it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf50b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function which given the output of a layer, ensures the sum of the values is one\n",
    "#it does this by computing the sum of the nodes and dividing each note by it\n",
    "def estimateAbundances(inputNodes,name):\n",
    "    sampleWiseSums=tf.keras.backend.sum(inputNodes,\n",
    "                                        axis=-1,\n",
    "                                        keepdims=True)\n",
    "    sampleWiseSums=tf.repeat(sampleWiseSums,\n",
    "                             sampleWiseSums.shape[-1],\n",
    "                             axis=-1)\n",
    "    return tf.math.divide(inputNodes,\n",
    "                          sampleWiseSums+1e-10,\n",
    "                          name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f22294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a fully connected layer to the network (output layer)\n",
    "regularElementStage1=tf.keras.layers.Dense(7,activation='relu')(regularElementFeatures)\n",
    "\n",
    "#add a normalization layer to the network\n",
    "regularAbundanceEmbedding=tf.nn.softmax(regularElementStage1,\n",
    "                                        name=\"Regular_Abundances\")\n",
    "\n",
    "\n",
    "#print shape of abundances\n",
    "print(f\"Unscaled regular element shape {regularAbundanceEmbedding.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77791108",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#add a hidden layer to the nuclear elements' encoder\n",
    "preElementNodes=tf.keras.layers.Dense(360,activation='relu')(nuclearElementFeatures)\n",
    "nuclearElementStage1=tf.keras.layers.Dense(3,activation=None)(preElementNodes)\n",
    "'''\n",
    "#get the abundances for the nuclear elements from their features\n",
    "nuclearElementStage1=tf.keras.layers.Dense(3,activation='relu')(nuclearElementFeatures)\n",
    "#compute absolute values\n",
    "nuclearAbundanceEmbedding=tf.nn.softmax(nuclearElementStage1,name=\"Nuclear_Abundances\")\n",
    "#print shape of abundances\n",
    "print(f\"Unscaled regular element shape {nuclearAbundanceEmbedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad853b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concattenate the two abundances\n",
    "fullAbundanceEmbedding=tf.concat([regularAbundanceEmbedding,nuclearAbundanceEmbedding],\n",
    "                                 axis=-1)\n",
    "\n",
    "print(f\"All element shape {fullAbundanceEmbedding.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bca4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a composite constraint to be applied to the weights of the hidden layers which are the weights\n",
    "class UnitNormNonNegetivityConstraint(tf.keras.constraints.Constraint):\n",
    "    def __init__(self,axis=0):\n",
    "        self.axis=axis\n",
    "    def __call__(self, w):\n",
    "        w=w*tf.cast(tf.greater_equal(w,0.0),\n",
    "                    tf.keras.backend.floatx())\n",
    "        \n",
    "        '''\n",
    "        w=w/(tf.keras.backend.epsilon()+tf.keras.backend.sqrt(tf.reduce_sum(tf.square(w),\n",
    "                                                                            axis=self.axis,\n",
    "                                                                            keepdims=True)\n",
    "                                                             )\n",
    "            )\n",
    "        '''\n",
    "        return w\n",
    "\n",
    "\n",
    "\n",
    "#create the composite constraint by combining the two constraints\n",
    "unitNormNonNegetivityConstraint=UnitNormNonNegetivityConstraint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d34f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the linear hidden layer as a combination of ten hidden layers each connected to a single abundance value\n",
    "elementWiseHiddenLayers=[]\n",
    "for i in range(fullAbundanceEmbedding.shape[-1]):\n",
    "    temp=tf.keras.layers.Dense(noOfChannels,activation=None,use_bias=False,kernel_constraint=unitNormNonNegetivityConstraint,name=allElementNames[i]+\"_Spectra\")(tf.expand_dims(fullAbundanceEmbedding[:,i],-1))\n",
    "    elementWiseHiddenLayers.append(tf.expand_dims(temp,-1))\n",
    "    del temp\n",
    "    print(f\"Shape of the spectra outputed by {allElementNames[i]} {elementWiseHiddenLayers[-1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0be980",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the element wise spectra\n",
    "linearMixedSpectra=tf.keras.backend.sum(tf.keras.layers.Concatenate(axis=-1)(elementWiseHiddenLayers),\n",
    "                                        axis=-1,\n",
    "                                        keepdims=False)\n",
    "print(f\"Shape of the linearly mixed spectra {linearMixedSpectra.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply two dense layers to the network\n",
    "nnGraph=tf.keras.layers.Dense(int(linearMixedSpectra.shape[-1]/2),activation='relu')(linearMixedSpectra)\n",
    "nnGraph=tf.keras.layers.Dense(int(linearMixedSpectra.shape[-1]/1.5),activation='relu')(nnGraph)\n",
    "nnGraph=tf.keras.layers.Dense(linearMixedSpectra.shape[-1],activation=None,name=\"Reconstruced_Spectra\")(nnGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dacffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model\n",
    "unmixingModel=tf.keras.Model(inputs=inputLayer,\n",
    "                             outputs=[regularAbundanceEmbedding,\n",
    "                                      nuclearAbundanceEmbedding,\n",
    "                                      nnGraph],\n",
    "                             name=\"Constrained_Dual_Encoder_Unmxing_Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the loss function for the embedding, Mean Squared Error\n",
    "abundanceLossFunction=tf.keras.losses.MeanSquaredError()\n",
    "#create the loss function for the reconstructed spectra, Cosine loss\n",
    "recontructedSpectraLoss=tf.keras.losses.CosineSimilarity()\n",
    "#create the optimizer\n",
    "optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "#set the number of epochs the model is to be trained for\n",
    "noOfEpochs=300\n",
    "#set the batch size\n",
    "batchSize=179\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbbd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "unmixingModel.compile(optimizer=optimizer,\n",
    "                      loss=[abundanceLossFunction,abundanceLossFunction,recontructedSpectraLoss],\n",
    "                      loss_weights=[1e-2,1,1e-5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0599e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the address where the untrained model will be saved\n",
    "untrainedModelAddress=\"C:/ML4Sci/Ml4Sci_GRS_abundance_estimation/Models/untrained_Truly_Constrainted_Dual_Unmixing_Model_With_PReLU.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d98edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the address where the untrained model will be saved\n",
    "untrainedModelWeightsAddress=\"C:/ML4Sci/Ml4Sci_GRS_abundance_estimation/Models/untrained_Truly_Constrainted_Dual_Unmixing_Model_With_PReLU_Weights.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66e51c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the untrained model\n",
    "unmixingModel.save_weights(untrainedModelWeightsAddress,\n",
    "                           overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ed2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the GPU is available\n",
    "if len(tf.config.list_physical_devices('GPU'))==1:\n",
    "    print(\"GPU available\")\n",
    "else:\n",
    "    print(\"GPU unavailable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e16479",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list to hold model training history\n",
    "modelTrainingHistories=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff452f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "noOfXticks=3\n",
    "noOfYticks=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310d4268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array to save the element-wise learnt spectra across folds\n",
    "foldWiseLearntElementalSpectra=np.zeros((noOfChannels,\n",
    "                                         fullAbundanceEmbedding.shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf74104",
   "metadata": {},
   "outputs": [],
   "source": [
    "unmixingModel.load_weights(untrainedModelWeightsAddress)\n",
    " #compile the model\n",
    "unmixingModel.compile(optimizer=optimizer,\n",
    "                      loss=[abundanceLossFunction,\n",
    "                            abundanceLossFunction,\n",
    "                            recontructedSpectraLoss],\n",
    "                      loss_weights=[1e-2,1,1e-5])\n",
    "\n",
    "#fit the model to the current fold's data\n",
    "currentModelTrainingHistory=unmixingModel.fit(x=preprocessedSpectra,\n",
    "                                                        y=[regularElementalAbundances,\n",
    "                                                           nuclearElementalAbundances,\n",
    "                                                           preprocessedSpectra],\n",
    "                                                        batch_size=batchSize,\n",
    "                                                        epochs=noOfEpochs)\n",
    "\n",
    "\n",
    "#save the training history of the current model\n",
    "modelTrainingHistories.append(currentModelTrainingHistory)\n",
    "\n",
    "\n",
    "\n",
    "#set the address where the untrained model will be saved\n",
    "trainedModelWeightsAddress=\"C:/ML4Sci/Ml4Sci_GRS_abundance_estimation/Models/trained_Constrainted_Dual_Unmixing_Model_With_PReLU_Weights_for_\"+str(i+1)+\"_Fold.h5\"\n",
    "\n",
    "#save the untrained model\n",
    "unmixingModel.save_weights(trainedModelWeightsAddress,\n",
    "                           overwrite=True)\n",
    "\n",
    "\n",
    "#iterate through all the elements\n",
    "for i in range(len(allElementNames)):\n",
    "\n",
    "    #get the mean spectrum for the current element\n",
    "    currentElementSpectrum=unmixingModel.get_layer(allElementNames[i]+\"_Spectra\").get_weights()[0][0]\n",
    "\n",
    "    #save the spectrum\n",
    "    foldWiseLearntElementalSpectra[:,i]=currentElementSpectrum\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fd9582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a figure \n",
    "figure,axes=plt.subplots(nrows=2,\n",
    "                         ncols=5,\n",
    "                         figsize=(5*5,5*2),\n",
    "                         dpi=500,\n",
    "                         sharex=True,\n",
    "                         sharey=True)\n",
    "\n",
    "\n",
    "#iterate through all the elements\n",
    "for i in range(len(allElementNames)):\n",
    "\n",
    "    #get the name of the current element\n",
    "    currentElementName=allElementNames[i]\n",
    "\n",
    "    #get the mean spectrum for the current element\n",
    "    currentElementSpectrum=foldWiseLearntElementalSpectra[:,i]\n",
    "\n",
    "    #plot the spectrum\n",
    "    axes[i//5,i%5].bar(relaventEnergyBands,\n",
    "                       currentElementSpectrum/np.linalg.norm(currentElementSpectrum),\n",
    "                       lw=10,\n",
    "                       width=10)\n",
    "\n",
    "    #add the title\n",
    "    axes[i//5,i%5].set_title(currentElementName,\n",
    "                             fontsize=baseFontSize*1.2)\n",
    "\n",
    "    #set the margin\n",
    "    axes[i//5,i%5].margins(0.01)\n",
    "\n",
    "    #add xticks and label\n",
    "    if i//5==1:\n",
    "        axes[i//5,i%5].set_xticks(np.arange(np.amin(relaventEnergyBands),\n",
    "                                            np.amax(relaventEnergyBands)+np.ptp(relaventEnergyBands)/noOfXticks,\n",
    "                                            np.ptp(relaventEnergyBands)/noOfXticks),\n",
    "                                  labels=np.round(np.arange(np.amin(relaventEnergyBands),\n",
    "                                                            np.amax(relaventEnergyBands)+np.ptp(relaventEnergyBands)/noOfXticks,\n",
    "                                                            np.ptp(relaventEnergyBands)/noOfXticks),\n",
    "                                                  0).astype('int'),\n",
    "                                  fontsize=baseFontSize*1.1)\n",
    "        axes[i//5,i%5].set_xlabel(\"KeV\",fontsize=baseFontSize*1.2)\n",
    "\n",
    "    #add yticks and label\n",
    "    if i%5==0:\n",
    "        axes[i//5,i%5].set_ylabel(\"log(Counts/min) ratioed\",fontsize=baseFontSize*1.2)\n",
    "\n",
    "    axes[i//5,i%5].set_yticks(np.arange(0,\n",
    "                                np.amax(currentElementSpectrum)+np.ptp(currentElementSpectrum)/noOfYticks,\n",
    "                                np.ptp(currentElementSpectrum)/noOfYticks),\n",
    "                      labels=np.round(np.arange(0,\n",
    "                                                np.amax(currentElementSpectrum)+np.ptp(currentElementSpectrum)/noOfYticks,\n",
    "                                                np.ptp(currentElementSpectrum)/noOfYticks),\n",
    "                                      2),\n",
    "                      fontsize=baseFontSize*1.1)\n",
    "#add a title\n",
    "figure.suptitle(f\"Element-wise Learnt spectra\",\n",
    "                fontsize=baseFontSize*1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3362580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import uniform_filter from scipy\n",
    "from scipy.ndimage import uniform_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908ffcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array to hold the smoothened element-wise spectra\n",
    "smoothenedElementWiseSpectra=np.zeros_like(foldWiseLearntElementalSpectra)\n",
    "\n",
    "#create a figure \n",
    "figure,axes=plt.subplots(nrows=2,\n",
    "                         ncols=5,\n",
    "                         figsize=(5*5,5*2),\n",
    "                         dpi=500,\n",
    "                         sharex=True,\n",
    "                         sharey=True)\n",
    "\n",
    "#iterate through the element-wise spectra and smoothen them\n",
    "for i in range(len(allElementNames)):\n",
    "    \n",
    "    #get the name of the current element\n",
    "    currentElementName=allElementNames[i]\n",
    "    \n",
    "    #get the mean spectrum for the current element\n",
    "    currentElementSpectrum=foldWiseLearntElementalSpectra[:,i]\n",
    "    \n",
    "    #normalize and smoothen the spectra\n",
    "    currentElementSpectrum=currentElementSpectrum/np.linalg.norm(currentElementSpectrum)\n",
    "    \n",
    "    #smoothen the spectrum\n",
    "    currentElementSpectrum=uniform_filter(currentElementSpectrum,\n",
    "                                          size=13)\n",
    "    #normalize the spectra\n",
    "    currentElementSpectrum=currentElementSpectrum/np.linalg.norm(currentElementSpectrum)\n",
    "    \n",
    "    #save the processed spectra\n",
    "    smoothenedElementWiseSpectra[:,i]=currentElementSpectrum\n",
    "    \n",
    "    #plot the spectrum\n",
    "    axes[i//5,i%5].plot(relaventEnergyBands,\n",
    "                        currentElementSpectrum,\n",
    "                        lw=2)\n",
    "\n",
    "    #add the title\n",
    "    axes[i//5,i%5].set_title(currentElementName,\n",
    "                             fontsize=baseFontSize*1.2)\n",
    "\n",
    "    #set the margin\n",
    "    axes[i//5,i%5].margins(0.01)\n",
    "\n",
    "    #add xticks and label\n",
    "    if i//5==1:\n",
    "        axes[i//5,i%5].set_xticks(np.arange(np.amin(relaventEnergyBands),\n",
    "                                            np.amax(relaventEnergyBands)+np.ptp(relaventEnergyBands)/noOfXticks,\n",
    "                                            np.ptp(relaventEnergyBands)/noOfXticks),\n",
    "                                  labels=np.round(np.arange(np.amin(relaventEnergyBands),\n",
    "                                                            np.amax(relaventEnergyBands)+np.ptp(relaventEnergyBands)/noOfXticks,\n",
    "                                                            np.ptp(relaventEnergyBands)/noOfXticks),\n",
    "                                                  0).astype('int'),\n",
    "                                  fontsize=baseFontSize*1.1)\n",
    "        axes[i//5,i%5].set_xlabel(\"KeV\",fontsize=baseFontSize*1.2)\n",
    "\n",
    "    #add yticks and label\n",
    "    if i%5==0:\n",
    "        axes[i//5,i%5].set_ylabel(\"log(Counts/min) ratioed\",fontsize=baseFontSize*1.2)\n",
    "\n",
    "    axes[i//5,i%5].set_yticks(np.arange(0,\n",
    "                                np.amax(currentElementSpectrum)+np.ptp(currentElementSpectrum)/noOfYticks,\n",
    "                                np.ptp(currentElementSpectrum)/noOfYticks),\n",
    "                      labels=np.round(np.arange(0,\n",
    "                                                np.amax(currentElementSpectrum)+np.ptp(currentElementSpectrum)/noOfYticks,\n",
    "                                                np.ptp(currentElementSpectrum)/noOfYticks),\n",
    "                                      2),\n",
    "                      fontsize=baseFontSize*1.1)\n",
    "#add a title\n",
    "figure.suptitle(f\"Smoothened Element-wise Learnt spectra\",\n",
    "                fontsize=baseFontSize*1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465309b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
